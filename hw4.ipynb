{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"hw4.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import urllib\n",
    "import scipy.optimize\n",
    "import random\n",
    "from collections import defaultdict # Dictionaries with default values\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem.porter import *\n",
    "from sklearn import linear_model\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "542338\n",
      "10000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "def parseData(fname):\n",
    "  for l in urllib.urlopen(fname):\n",
    "    yield ast.literal_eval(l)\n",
    "    \n",
    "def parseDataFromFile(fname):\n",
    "  for l in open(fname):\n",
    "    yield ast.literal_eval(l)   \n",
    "    \n",
    "data_ = list(parseDataFromFile(\"goodreads_reviews_comics_graphic.json\"))\n",
    "print(len(data_))\n",
    "data = data_[:20000]\n",
    "train = data[:10000]\n",
    "test = data[10000:]\n",
    "print(len(train))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"problem1.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "{'user_id': 'dc3763cdb9b2cae805882878eebb6a32', 'book_id': '18471619', 'review_id': '66b2ba840f9bd36d6d27f46136fe4772', 'rating': 3, 'review_text': 'Sherlock Holmes and the Vampires of London \\n Release Date: April 2014 \\n Publisher: Darkhorse Comics \\n Story by: Sylvain Cordurie \\n Art by: Laci \\n Colors by: Axel Gonzabo \\n Cover by: Jean Sebastien Rossbach \\n ISDN: 9781616552664 \\n MSRP: $17.99 Hardcover \\n \"Sherlock Holmes died fighting Professor Moriarty in the Reichenbach Falls. \\n At least, that\\'s what the press claims. \\n However, Holmes is alive and well and taking advantage of his presumed death to travel the globe. \\n Unfortunately, Holmes\\'s plans are thwarted when a plague of vampirism haunts Britain. \\n This book collects Sherlock Holmes and the Vampires of London Volumes 1 and 2, originally created by French publisher Soleil.\" - Darkhorse Comics \\n When I received this copy of \"Sherlock Holmes and the Vampires of London\" I was Ecstatic! The cover art was awesome and it was about two of my favorite things, Sherlock Holmes and Vampires. I couldn\\'t wait to dive into this! \\n Unfortunately, that is where my excitement ended. The story takes place a month after Sherlock Holmes supposed death in his battle with Professor Moriarty. Sherlock\\'s plan to stay hidden and out of site are ruined when on a trip with his brother Mycroft, they stumble on the presence of vampires. That is about as much of Sherlock\\'s character that comes through the book. I can\\'t even tell you the story really because nothing and I mean nothing stuck with me after reading it. I never, ever got the sense of Sherlock Holmes anywhere in this graphic novel, nor any real sense of mystery or crime. It was just Sherlock somehow battling vampires that should have had absolutely no trouble snuffing him out in a fight, but somehow always surviving and holding his own against supernatural, super powerful, blazingly fast creatures. \\n The cover art is awesome and it truly made me excited to read this but everything else feel completely flat for me. I tried telling myself that \"it\\'s a graphic novel, it would be hard to translate mystery, details, emotion\" but then I remembered reading DC Comic\\'s \"Identity Crisis\" and realized that was a load of crap. I know it\\'s unfair to compare the two as \"Identity Crisis\" had popular mystery author Brad Meltzer writing it right? Yeah....no. The standard was set that day and there is more than enough talent out there to create a great story in a graphic novel. \\n That being said, it wasn\\'t a horrible story, it just didn\\'t grip me for feel anything like Sherlock Holmes to me. It was easy enough to follow but I felt no sense of tension, stakes or compassion for any of the characters. \\n As far as the vampires go, it\\'s hard to know what to expect anymore as there are so many different versions these days. This was the more classic version which I personally prefer, but again I didn\\'t find anything that portrayed their dominance, calm confidence or sexuality. There was definitely a presence of their physical prowess but somehow that was lost on me as easily as Sherlock was able to defend himself. I know it, wouldn\\'t do to kill of the main character, but this would have a been a great opportunity to build around the experience and beguiling nature of a vampire that had lived so many years of experience. Another chance to showcase Sherlock\\'s intellect in a battle of wits over strength in something more suitable for this sort of story as apposed to trying to make it feel like an action movie. \\n Maybe I expected to much and hoped to have at least a gripping premise or some sort of interesting plot or mystery but I didn\\'t find it here. This may be a must have for serious Sherlock Holmes fans that have to collect everything about him, but if you are looking for a great story inside a graphic novel, I would have to say pass on this one. \\n That artwork is good, cover is great, story is lacking so I am giving it 2.5 out of 5 stars.', 'date_added': 'Thu Dec 05 10:44:25 -0800 2013', 'date_updated': 'Thu Dec 05 10:45:15 -0800 2013', 'read_at': 'Tue Nov 05 00:00:00 -0800 2013', 'started_at': '', 'n_votes': 0, 'n_comments': 0}\n",
      "1814797\n",
      "1794812\n"
     ]
    }
   ],
   "source": [
    "wordCount = defaultdict(int)\n",
    "word2Count = defaultdict(int)\n",
    "totalWords = 0\n",
    "totalWords2 = 0\n",
    "punct = string.punctuation\n",
    "print(punct)\n",
    "print(data[0])\n",
    "\n",
    "for d in data:\n",
    "    t = d['review_text']\n",
    "    t = t.lower() # lowercase string\n",
    "    t = [c for c in t if not (c in punct)] # non-punct characters\n",
    "    t = ''.join(t) # convert back to string\n",
    "    words = t.strip().split() # tokenizes\n",
    "    words2 = [' '.join(x) for x in list(zip(words[:-1],words[1:]))] \n",
    "    for w in words:\n",
    "        totalWords += 1\n",
    "        wordCount[w] += 1\n",
    "        \n",
    "    for w in words2:\n",
    "        totalWords2 += 1\n",
    "        word2Count[w] += 1\n",
    "        \n",
    "print(totalWords)\n",
    "print(totalWords2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59814\n",
      "[(100875, 'the'), (52202, 'and'), (49717, 'a'), (46551, 'of'), (41950, 'to'), (34692, 'i'), (30131, 'is'), (24381, 'this'), (24158, 'it'), (23899, 'in')]\n",
      "[(22, 'prominent'), (22, 'primary'), (22, 'powered'), (22, 'poco'), (22, 'plein'), (22, 'photo'), (22, 'personagens'), (22, 'pause'), (22, 'patient'), (22, 'outfit')]\n",
      "528310\n",
      "[(12580, 'of the'), (6821, 'in the'), (4887, 'the story'), (4625, 'and the'), (4472, 'is a'), (4055, 'to the'), (3379, 'this is'), (3157, 'to be'), (2785, 'it was'), (2781, 'with the')]\n",
      "[(37, 'well told'), (37, 'was disappointed'), (37, 'war ii'), (37, 'wanted more'), (37, 'violent and'), (37, 'used in'), (37, 'track down'), (37, 'to volume'), (37, 'to spoil'), (37, 'to rate')]\n",
      "[(100875, 'the'), (52202, 'and'), (49717, 'a'), (46551, 'of'), (41950, 'to'), (34692, 'i'), (30131, 'is'), (24381, 'this'), (24158, 'it'), (23899, 'in')]\n",
      "[(65, 'discovered'), (65, 'develop'), (65, 'destruction'), (65, 'combined'), (65, 'cliched'), (65, 'characterization of'), (65, 'cases'), (65, 'calls'), (65, 'but his'), (65, 'but all')]\n",
      "['the', 'and', 'a', 'of', 'to', 'i', 'is', 'this', 'it', 'in']\n",
      "['of the', 'in the', 'the story', 'and the', 'is a', 'to the', 'this is', 'to be', 'it was', 'with the']\n",
      "['the', 'and', 'a', 'of', 'to', 'i', 'is', 'this', 'it', 'in', 'that', 'but', 'was', 'with', 'as', 'story', 'for', 'of the', 'its', 'on', 'are', 'not', 'have', 'you', 'so', 'be', 'one', 'book', 'read', 'in the', 'more', 'like', 'all', 'at', 'an', 'his', 'just', 'really', 'about', 'from', 'me', 'some', 'my', 'he', 'up', 'what', 'her', 'good', 'the story', 'by']\n"
     ]
    }
   ],
   "source": [
    "print(len(wordCount))\n",
    "counts = [(wordCount[w], w) for w in wordCount]\n",
    "counts.sort()\n",
    "counts.reverse()\n",
    "print(counts[:10])\n",
    "print(counts[5000:5010])\n",
    "\n",
    "print(len(word2Count))\n",
    "counts2 = [(word2Count[w], w) for w in word2Count]\n",
    "counts2.sort()\n",
    "counts2.reverse()\n",
    "print(counts2[:10])\n",
    "print(counts2[5000:5010])\n",
    "\n",
    "countsadd = counts\n",
    "countsadd.extend(counts2)\n",
    "countsadd.sort()\n",
    "countsadd.reverse()\n",
    "print(countsadd[:10])\n",
    "print(countsadd[5000:5010])\n",
    "\n",
    "unigrams = [w[1] for w in counts[:1000]]\n",
    "print(unigrams[:10])\n",
    "unigramId = dict(zip(unigrams, range(len(unigrams))))\n",
    "unigramSet = set(unigrams)\n",
    "\n",
    "bigrams = [w[1] for w in counts2[:1000]]\n",
    "print(bigrams[:10])\n",
    "bigramId = dict(zip(bigrams, range(len(bigrams))))\n",
    "bigramSet = set(bigrams)\n",
    "\n",
    "combined = [w[1] for w in countsadd[:1000]]\n",
    "print(combined[:50])\n",
    "combinedId = dict(zip(combined, range(len(combined))))\n",
    "combinedSet = set(combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_unigram(datum):\n",
    "    feat = [0]*len(unigrams)\n",
    "    r = ''.join([c for c in datum['review_text'].lower() if not c in punct])\n",
    "    ws = r.split()\n",
    "    \n",
    "    for w in ws :\n",
    "        if w in unigrams:\n",
    "            feat[unigramId[w]] += 1\n",
    "    feat.append(1) #offset\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_bigram(datum):\n",
    "    feat = [0]*len(bigrams)\n",
    "    r = ''.join([c for c in datum['review_text'].lower() if not c in punct])\n",
    "    ws = r.split()\n",
    "    ws2 = [' '.join(x) for x in list(zip(ws[:-1],ws[1:]))]\n",
    "   \n",
    "    for w in ws2 :\n",
    "        if w in bigrams:\n",
    "            feat[bigramId[w]] += 1\n",
    "    feat.append(1) #offset\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_combined(datum):\n",
    "    feat = [0]*len(combined)\n",
    "    r = ''.join([c for c in datum['review_text'].lower() if not c in punct])\n",
    "    ws = r.split()\n",
    "    ws2 = [' '.join(x) for x in list(zip(ws[:-1],ws[1:]))]\n",
    "\n",
    "    for w in ws + ws2 :\n",
    "        if w in combined:\n",
    "            feat[combinedId[w]] += 1\n",
    "    feat.append(1) #offset\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 16, 18, 23, 21, 17, 8, 10, 13, 7, 13, 9, 11, 3, 8, 8, 6, 0, 3, 4]\n",
      "[-0.00457135  0.02714538 -0.00551991 -0.00079496  0.01669224 -0.01136056\n",
      " -0.00395026 -0.00347088 -0.009181    0.01306438  0.01329566 -0.03784267\n",
      " -0.0252959  -0.0184116   0.02904519 -0.04263389 -0.01868626  0.\n",
      " -0.00255318  0.03551502]\n",
      "MSEtrain =  0.8366988157914573\n",
      "MSEtest =  1.2175286628938813\n",
      "[(0.16980834842572978, 'graphic'), (0.05298929605128229, 'and'), (0.046125233949278255, 'did'), (0.040575182490206335, 'fear'), (0.03551501622887232, 'with')]\n",
      "[(-0.03784267266626414, 'like'), (-0.039731448165314126, 'an'), (-0.042633890048506984, 'kill'), (-0.05083494692111321, 'and'), (-0.12129747395215885, 'many')]\n"
     ]
    }
   ],
   "source": [
    "# unigram\n",
    "X_train = [feature_unigram(d) for d in train]\n",
    "y_train = [d['rating'] for d in train]\n",
    "print(X_train[0][:20])\n",
    "X_test = [feature_unigram(d) for d in test]\n",
    "y_test = [d['rating'] for d in test]\n",
    "\n",
    "\n",
    "clf = linear_model.Ridge(1.0, fit_intercept=False) # MSE + 1.0 l2\n",
    "clf.fit(X_train, y_train)\n",
    "theta = clf.coef_\n",
    "pred_train = clf.predict(X_train)\n",
    "MSEtrain = sum((y_train - pred_train)**2)/len(y_train)\n",
    "pred_test = clf.predict(X_test)\n",
    "MSEtest = sum((y_test - pred_test)**2)/len(y_test)\n",
    "print(theta[:20])\n",
    "print(\"MSEtrain = \",MSEtrain)\n",
    "print(\"MSEtest = \",MSEtest)\n",
    "\n",
    "weights = list(zip(theta, words + ['constant_feat']))\n",
    "weights.sort()\n",
    "weights.reverse()\n",
    "print(weights[:5])\n",
    "print(weights[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 2, 3, 0, 0, 0, 0, 3, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1]\n",
      "[ 0.01311357 -0.0175345  -0.06386067  0.01209601  0.02495477  0.01678908\n",
      "  0.04453342 -0.03019126 -0.01181329 -0.02630695  0.0032872   0.00020249\n",
      "  0.04096715  0.0414431   0.06604158  0.04040725  0.01215056 -0.02004027\n",
      "  0.01831881  0.02489957]\n",
      "MSEtrain =  1.0065412723734712\n",
      "MSEtest =  1.1220572272163036\n",
      "[(0.14191392729040753, 'reassuring'), (0.0851562815472533, 'it'), (0.06604158286633115, 'i'), (0.05882911866021295, 'but'), (0.05640433763721195, 'graphic')]\n",
      "[(-0.06961567690231196, 'kind'), (-0.08464565450241211, 'novel'), (-0.10207313941865265, 'so'), (-0.11931400125694432, 'inadequacy'), (-0.1336222036339421, 'hope')]\n"
     ]
    }
   ],
   "source": [
    "# bigram\n",
    "X_train = [feature_bigram(d) for d in data]\n",
    "y_train = [d['rating'] for d in data]\n",
    "print(X_train[0][:20])\n",
    "X_test = [feature_bigram(d) for d in test]\n",
    "y_test = [d['rating'] for d in test]\n",
    "\n",
    "\n",
    "clf = linear_model.Ridge(1.0, fit_intercept=False) # MSE + 1.0 l2\n",
    "clf.fit(X_train, y_train)\n",
    "theta = clf.coef_\n",
    "pred_train = clf.predict(X_train)\n",
    "MSEtrain = sum((y_train - pred_train)**2)/len(y_train)\n",
    "pred_test = clf.predict(X_test)\n",
    "MSEtest = sum((y_test - pred_test)**2)/len(y_test)\n",
    "print(theta[:20])\n",
    "print(\"MSEtrain = \",MSEtrain)\n",
    "print(\"MSEtest = \",MSEtest)\n",
    "\n",
    "weights = list(zip(theta, words + ['constant_feat']))\n",
    "weights.sort()\n",
    "weights.reverse()\n",
    "print(weights[:5])\n",
    "print(weights[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 16, 18, 23, 21, 17, 8, 10, 13, 7, 13, 9, 11, 3, 8, 8, 6, 2, 3, 4]\n",
      "[ 0.00250727  0.01543956  0.00369686  0.00560763  0.0112723   0.00771787\n",
      " -0.00843389 -0.0059452   0.0012322   0.00744561  0.00133197 -0.04881002\n",
      " -0.04028023 -0.00953129  0.01934328 -0.01307313 -0.03214375 -0.00487283\n",
      " -0.00175648  0.00416778]\n",
      "MSEtrain =  0.9494877022028863\n",
      "MSEtest =  1.0534730062253814\n",
      "[(0.17720752773970394, 'graphic'), (0.06759523019416336, 'and'), (0.042655790155269345, 'did'), (0.039480098883190866, 'great'), (0.03297489843505927, 'teens')]\n",
      "[(-0.041554104888092115, 'an'), (-0.04375117959870784, 'and'), (-0.04881001987645827, 'like'), (-0.0539720160958572, 'inspiration'), (-0.12737492382679572, 'many')]\n"
     ]
    }
   ],
   "source": [
    "# combined\n",
    "X_train = [feature_combined(d) for d in data]\n",
    "y_train = [d['rating'] for d in data]\n",
    "print(X_train[0][:20])\n",
    "X_test = [feature_combined(d) for d in test]\n",
    "y_test = [d['rating'] for d in test]\n",
    "\n",
    "\n",
    "\n",
    "clf = linear_model.Ridge(1.0, fit_intercept=False) # MSE + 1.0 l2\n",
    "clf.fit(X_train, y_train)\n",
    "theta = clf.coef_\n",
    "pred_train = clf.predict(X_train)\n",
    "MSEtrain = sum((y_train - pred_train)**2)/len(y_train)\n",
    "pred_test = clf.predict(X_test)\n",
    "MSEtest = sum((y_test - pred_test)**2)/len(y_test)\n",
    "print(theta[:20])\n",
    "print(\"MSEtrain = \",MSEtrain)\n",
    "print(\"MSEtest = \",MSEtest)\n",
    "\n",
    "weights = list(zip(theta, words + ['constant_feat']))\n",
    "weights.sort()\n",
    "weights.reverse()\n",
    "print(weights[:5])\n",
    "print(weights[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans for Problem1 :\n",
    "\n",
    "a. unigram feature \n",
    "\n",
    "   MSEtrain =  0.8366988157914573\n",
    "   \n",
    "   MSEtest =  1.2175286628938813\n",
    "   \n",
    "   five most positive tokens : [(0.16980834842572978, 'graphic'), (0.05298929605128229, 'and'), (0.046125233949278255, 'did'), (0.040575182490206335, 'fear'), (0.03551501622887232, 'with')]\n",
    "   \n",
    "   five most negative tokens : [(-0.03784267266626414, 'like'), (-0.039731448165314126, 'an'), (-0.042633890048506984, 'kill'), (-0.05083494692111321, 'and'), (-0.12129747395215885, 'many')]\n",
    "\n",
    "b. bigram feature\n",
    "\n",
    "   MSEtrain =  1.0065412723734712\n",
    "   \n",
    "   MSEtest =  1.1220572272163036\n",
    "   \n",
    "   five most positive tokens : [(0.14191392729040753, 'reassuring'), (0.0851562815472533, 'it'), (0.06604158286633115, 'i'), (0.05882911866021295, 'but'), (0.05640433763721195, 'graphic')]\n",
    "   \n",
    "   five most negative tokens : [(-0.06961567690231196, 'kind'), (-0.08464565450241211, 'novel'), (-0.10207313941865265, 'so'), (-0.11931400125694432, 'inadequacy'), (-0.1336222036339421, 'hope')]\n",
    "\n",
    "c. combined feature\n",
    "\n",
    "   MSEtrain =  0.9494877022028863 \n",
    "   \n",
    "   MSEtest =  1.0534730062253814\n",
    "   \n",
    "   five most positive tokens : [(0.17720752773970394, 'graphic'), (0.06759523019416336, 'and'), (0.042655790155269345, 'did'), (0.039480098883190866, 'great'), (0.03297489843505927, 'teens')]\n",
    "   \n",
    "   five most negative tokens : [(-0.041554104888092115, 'an'), (-0.04375117959870784, 'and'), (-0.04881001987645827, 'like'), (-0.0539720160958572, 'inspiration'), (-0.12737492382679572, 'many')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"problem2.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "unigrams = [w[1] for w in counts[:1000]]\n",
    "# tf-idf\n",
    "df = defaultdict(int)\n",
    "for d in train:\n",
    "    r = ''.join([c for c in d['review_text'].lower() if not c in punct])\n",
    "    for w in set(r.split()):\n",
    "        if w in unigrams:\n",
    "            df[w] += 1\n",
    "\n",
    "rev = train[0]\n",
    "tf = defaultdict(int)\n",
    "r = ''.join([c for c in rev['review_text'].lower() if not c in punct])\n",
    "for w in r.split():\n",
    "    if w in unigrams:\n",
    "        tf[w] += 1\n",
    "    \n",
    "tfidfQuery = [0]*1000\n",
    "for i, w in enumerate(unigrams):\n",
    "    if df[w]!=0:\n",
    "        tfidfQuery[i] = tf[w]*math.log2(len(train)/df[w])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.5138592638638093, 0.5078040760482171, 0.5023689098425748, 0.48664311392282844, 0.47870877498155306, 0.47870877498155306, 0.47870877498155306, 0.47870877498155306, 0.47870877498155306]\n",
      "(0.5138592638638093, '2908fbc62d0e573c9032424792c30663', 'Originally posted on My Book Musings. \\n *Copy provided by launch event organizer for an honest review. \\n Anak Bathala: Kalem is the first book in a five-book graphic novel series, revolving around mythology of the old times. The first book is about Kalem\\'s search for why he is called \"Anak Bathala\" (demi-god). The language in the book has a mixture of English and Filipino terms, with Baybayin and Surat Mangyan scripts written in some parts of the book. \\n A brief overview of the whole series from their Facebook page: \\n Anak Bathala is the epic adventure of Kalem that showcases rich Filipino mythology and culture. It exhibits native Filipino beliefs and folk work intertwined with values. The other books included in the series collection are: Yamal, Arau, Yesha and Anak Bathala. Each character embodies intrinsic Filipino traits like admirable virtues of courage, determination, self-discipline, prudence, camaraderie and leadership. Their stories also uncover the deceiving sphere of glorious power. The elven persona of Yesha exudes the remarkable attributes of womanhood that represents Filipino women. Anak Bathala pays homage to the golden age of comic makers with its ground of artistic and detailed illustrations. \\n The final book of Anak Bathala juxtaposes the revolving characters of books 1-4 and how they will join together to overcome the evil transgression of Haring Nannum and Bathala Karimlan in the Land of Mystical Mindoro. \\n -oOo- \\n The story is fast-paced and interesting, but the story was too short for me to truly grasp Kalem\\'s character. But from what I have read so far, he seems nice, responsible, and brave. I thought he was going to have a love interest with his childhood friend, but reading on, I was thinking that it would be slightly hard for him to have a human relationship, especially if he has to go off into a battle. \\n I\\'m the type of reader who likes learning new things, whether they be fact or fiction. At first, I did not like the scripts used because I kept having to translate them myself. I was on page nine before I discovered that there are Filipino and English translations on pages 122 to 123. Gah. Silly me! I must say, though, that the scripts are easy to understand after a couple of pages of going back and forth. If you\\'re fluent in Filipino, I suggest you enjoy the scripts and translate them yourself. I think it\\'s part of the whole enjoying-the-graphic-novel process. \\n My primary beef with the graphic novel is that it is too dark to see the illustrations. I\\'m sure the illustrations are very nice, but too often, I had to strain my eyes to see the outlines. It made it hard for me to read and appreciate the graphic novel as a whole. The cover is nice, though. While the background is very dark, Kalem stands out, which I think is the whole point as he is the focus of book 1. \\n One thing that I did notice though is that Kalem seemed to grow older as the story progressed. I actually liked it, because I felt like the strain of the battle, the pain of losing Ba\\' was showing on his face. I felt the weight of their expectations upon him, and it was nice that these were reflected on his face. I don\\'t know if that was the intent of the creators, but it sure was a nice touch. \\n One part that confused me is the presence of the woman at the start of the novel. Though by the end of the book I pretty much knew, but it still would have been nice to know her, because I was confused as to why there is a woman there, amidst death and war. Also, by the end of the book, I wondered what happened to the mutia that Ba\\' Magiting got. Remember that? I don\\'t think it was mentioned in the story that Kalem has it, or what it is for. \\n Other things I noticed is that there are some errors such as in the line \"The panganay left Kalualhatian\". I think it lacks the word \"for\" in between left and Kalualhatian, because the idea is that he joined Bathala in Kalualhatian, leaving this world behind. Another is that there was a misplaced apostrophe in \"Detinos\\' reached further...\" \\n I\\'m hoping the next books would have more dialogue and explanations, as well as clearer images. There\\'s an excerpt of Kalem in their website and the graphics are gorgeous! I hope they\\'ll publish that edition. I\\'m not sure if it was just with my copy, but mine already had a few loose pages. I\\'m hoping that\\'s just in my case! \\n Am I going to get the next book? Probably! Because my attention was already grabbed by Anak Bathala: Kalem and I started to get curious as to what happens, and how he came to be on earth. Being a demi god, isn\\'t he supposed to be uh...somewhere else? So I\\'m really curious as to how his journey as and what his role is in the big picture. \\n If you\\'re a fan of mythology, Filipino fiction, looking for something new, a little bit of history mixed with fantasy, do be sure to check this out. Anak Bathala: Kalem is available at Fully Booked branches nationwide.')\n"
     ]
    }
   ],
   "source": [
    "# cosine similarity \n",
    "def Cosine(x1,x2):\n",
    "    numer = 0\n",
    "    norm1 = 0\n",
    "    norm2 = 0\n",
    "    for a1,a2 in zip(x1,x2):\n",
    "        numer += a1*a2\n",
    "        norm1 += a1**2\n",
    "        norm2 += a2**2\n",
    "    if norm1*norm2:\n",
    "        return numer / math.sqrt(norm1*norm2)\n",
    "    return 0\n",
    "    \n",
    "similarities = []\n",
    "for rev2 in train:\n",
    "    tf = defaultdict(int)#date_updated\n",
    "    r = ''.join([c for c in rev2['review_text'].lower() if not c in punct])\n",
    "    for w in r.split():\n",
    "        if w in unigrams:\n",
    "            tf[w] += 1\n",
    "    tfidf2 = [0]*1000\n",
    "    for i, w in enumerate(unigrams):\n",
    "        if df[w]!=0:\n",
    "            tfidf2[i] = tf[w]*math.log2(len(train)/df[w])\n",
    "    similarities.append((Cosine(tfidfQuery, tfidf2), rev2['review_id'] ,rev2['review_text']))\n",
    "    \n",
    "similarities.sort(reverse=True)\n",
    "print([ similarities[i][0] for i in range(10) ])\n",
    "print(similarities[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans for Problem2 :\n",
    "\n",
    "a.  review ID that has the highest cosine similarity compared to the first review : '2908fbc62d0e573c9032424792c30663'\n",
    "    \n",
    "    text : 'Originally posted on My Book Musings. \\n *Copy provided by launch event organizer for an honest review. \\n Anak Bathala: Kalem is the first book in a five-book graphic novel series, revolving around mythology of the old times. The first book is about Kalem\\'s search for why he is called \"Anak Bathala\" (demi-god). The language in the book has a mixture of English and Filipino terms, with Baybayin and Surat Mangyan scripts written in some parts of the book. \\n A brief overview of the whole series from their Facebook page: \\n Anak Bathala is the epic adventure of Kalem that showcases rich Filipino mythology and culture. It exhibits native Filipino beliefs and folk work intertwined with values. The other books included in the series collection are: Yamal, Arau, Yesha and Anak Bathala. Each character embodies intrinsic Filipino traits like admirable virtues of courage, determination, self-discipline, prudence, camaraderie and leadership. Their stories also uncover the deceiving sphere of glorious power. The elven persona of Yesha exudes the remarkable attributes of womanhood that represents Filipino women. Anak Bathala pays homage to the golden age of comic makers with its ground of artistic and detailed illustrations. \\n The final book of Anak Bathala juxtaposes the revolving characters of books 1-4 and how they will join together to overcome the evil transgression of Haring Nannum and Bathala Karimlan in the Land of Mystical Mindoro. \\n -oOo- \\n The story is fast-paced and interesting, but the story was too short for me to truly grasp Kalem\\'s character. But from what I have read so far, he seems nice, responsible, and brave. I thought he was going to have a love interest with his childhood friend, but reading on, I was thinking that it would be slightly hard for him to have a human relationship, especially if he has to go off into a battle. \\n I\\'m the type of reader who likes learning new things, whether they be fact or fiction. At first, I did not like the scripts used because I kept having to translate them myself. I was on page nine before I discovered that there are Filipino and English translations on pages 122 to 123. Gah. Silly me! I must say, though, that the scripts are easy to understand after a couple of pages of going back and forth. If you\\'re fluent in Filipino, I suggest you enjoy the scripts and translate them yourself. I think it\\'s part of the whole enjoying-the-graphic-novel process. \\n My primary beef with the graphic novel is that it is too dark to see the illustrations. I\\'m sure the illustrations are very nice, but too often, I had to strain my eyes to see the outlines. It made it hard for me to read and appreciate the graphic novel as a whole. The cover is nice, though. While the background is very dark, Kalem stands out, which I think is the whole point as he is the focus of book 1. \\n One thing that I did notice though is that Kalem seemed to grow older as the story progressed. I actually liked it, because I felt like the strain of the battle, the pain of losing Ba\\' was showing on his face. I felt the weight of their expectations upon him, and it was nice that these were reflected on his face. I don\\'t know if that was the intent of the creators, but it sure was a nice touch. \\n One part that confused me is the presence of the woman at the start of the novel. Though by the end of the book I pretty much knew, but it still would have been nice to know her, because I was confused as to why there is a woman there, amidst death and war. Also, by the end of the book, I wondered what happened to the mutia that Ba\\' Magiting got. Remember that? I don\\'t think it was mentioned in the story that Kalem has it, or what it is for. \\n Other things I noticed is that there are some errors such as in the line \"The panganay left Kalualhatian\". I think it lacks the word \"for\" in between left and Kalualhatian, because the idea is that he joined Bathala in Kalualhatian, leaving this world behind. Another is that there was a misplaced apostrophe in \"Detinos\\' reached further...\" \\n I\\'m hoping the next books would have more dialogue and explanations, as well as clearer images. There\\'s an excerpt of Kalem in their website and the graphics are gorgeous! I hope they\\'ll publish that edition. I\\'m not sure if it was just with my copy, but mine already had a few loose pages. I\\'m hoping that\\'s just in my case! \\n Am I going to get the next book? Probably! Because my attention was already grabbed by Anak Bathala: Kalem and I started to get curious as to what happens, and how he came to be on earth. Being a demi god, isn\\'t he supposed to be uh...somewhere else? So I\\'m really curious as to how his journey as and what his role is in the big picture. \\n If you\\'re a fan of mythology, Filipino fiction, looking for something new, a little bit of history mixed with fantasy, do be sure to check this out. Anak Bathala: Kalem is available at Fully Booked branches nationwide.'\n",
    "        \n",
    "    cosine similarity : 0.5138592638638093"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"problem3.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (a) Latent Factor Model with user and item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import random\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "from collections import defaultdict\n",
    "from fastFM import als\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom dateutil.parser import parse\\n\\n## user only appear successively\\n## date added is from new to old for each user\\ncur_user = data[0][\\'user_id\\']\\ncur_date = data[0][\\'date_added\\']\\nuserIDs = {}\\ncnt = 0\\nfor d in data:\\n    #print(cnt)\\n    u = d[\\'user_id\\']\\n    du = d[\\'date_added\\']\\n    #if (u==\\'1e946b8f76d5a75414946767cd18cff9\\'): print(cnt)\\n    if (u!=cur_user and u in userIDs): print(\"%d!!! %s %s\"%(cnt, cur_user,u))\\n    if (u==cur_user and parse(d[\\'date_added\\'])>parse(cur_date)): print(\"%d!!! %s %s\"%(cnt, parse(d[\\'date_added\\']),parse(cur_date)))\\n    cur_user = u\\n    cur_date = du\\n    if not u in userIDs: userIDs[u] = len(userIDs)\\n    cnt+=1\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from dateutil.parser import parse\n",
    "\n",
    "## user only appear successively\n",
    "## date added is from new to old for each user\n",
    "cur_user = data[0]['user_id']\n",
    "cur_date = data[0]['date_added']\n",
    "userIDs = {}\n",
    "cnt = 0\n",
    "for d in data:\n",
    "    #print(cnt)\n",
    "    u = d['user_id']\n",
    "    du = d['date_added']\n",
    "    #if (u=='1e946b8f76d5a75414946767cd18cff9'): print(cnt)\n",
    "    if (u!=cur_user and u in userIDs): print(\"%d!!! %s %s\"%(cnt, cur_user,u))\n",
    "    if (u==cur_user and parse(d['date_added'])>parse(cur_date)): print(\"%d!!! %s %s\"%(cnt, parse(d['date_added']),parse(cur_date)))\n",
    "    cur_user = u\n",
    "    cur_date = du\n",
    "    if not u in userIDs: userIDs[u] = len(userIDs)\n",
    "    cnt+=1\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nUsers = 636, nItems = 11892\n",
      "len(train) = 18291, len(test) = 636\n"
     ]
    }
   ],
   "source": [
    "import dateutil.parser\n",
    "userCount = {}\n",
    "for d in data:\n",
    "    u = d['user_id']\n",
    "    if not u in userCount: userCount[u] = 1\n",
    "    else : userCount[u] +=1\n",
    "        \n",
    "DataPerUser = defaultdict(list)\n",
    "userIDs,itemIDs = {},{}\n",
    "for d in data:\n",
    "    u = d['user_id']\n",
    "    i = d['book_id']\n",
    "    t = d['date_added']\n",
    "    r = d['rating']\n",
    "    dt = dateutil.parser.parse(t)\n",
    "    t = int(dt.timestamp())\n",
    "    #print(t)\n",
    "    if userCount[u]<3 : continue\n",
    "    if not u in userIDs: userIDs[u] = len(userIDs)\n",
    "    if not i in itemIDs: itemIDs[i] = len(itemIDs)\n",
    "    DataPerUser[u].append((t,u,i,r))\n",
    "nUsers,nItems = len(userIDs),len(itemIDs)\n",
    "print(\"nUsers = %d, nItems = %d\"%(nUsers,nItems))\n",
    "\n",
    "train = []\n",
    "test = []\n",
    "for u in DataPerUser:\n",
    "    DataPerUser[u].sort()\n",
    "    length = len(DataPerUser[u])\n",
    "    for i in range(length-1):\n",
    "        train.append(DataPerUser[u][i])\n",
    "    test.append(DataPerUser[u][length-1]) \n",
    "\n",
    "print(\"len(train) = %d, len(test) = %d\"%(len(train),len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE train =  0.013160128282292967\n",
      "MSE test =  1.3137676309457482\n"
     ]
    }
   ],
   "source": [
    "import fastFM\n",
    "X_train = scipy.sparse.lil_matrix((len(train), nUsers + nItems))\n",
    "for i in range(len(train)): # (t,u,i,r)\n",
    "    user = userIDs[train[i][1]]\n",
    "    item = itemIDs[train[i][2]]\n",
    "    X_train[i,user] = 1 # One-hot encoding of user\n",
    "    X_train[i,nUsers + item] = 1 # One-hot encoding of item\n",
    "y_train = numpy.array([d[3] for d in train])\n",
    "\n",
    "X_test = scipy.sparse.lil_matrix((len(test), nUsers + nItems))\n",
    "for i in range(len(test)):\n",
    "    user = userIDs[test[i][1]]\n",
    "    item = itemIDs[test[i][2]]\n",
    "    X_test[i,user] = 1 # One-hot encoding of user\n",
    "    X_test[i,nUsers + item] = 1 # One-hot encoding of item\n",
    "y_test = numpy.array([d[3] for d in test])\n",
    "        \n",
    "def MSE(predictions, labels):\n",
    "    differences = [(x-y)**2 for x,y in zip(predictions,labels)]\n",
    "    return sum(differences) / len(differences)\n",
    "\n",
    "fm = fastFM.als.FMRegression(n_iter=1000, init_stdev=0.1, rank=5, l2_reg_w=0.1, l2_reg_V=0.5)\n",
    "fm.fit(X_train, y_train)\n",
    "y_pred = fm.predict(X_train)\n",
    "print(\"MSE train = \",MSE(y_pred, y_train))       \n",
    "                     \n",
    "y_pred = fm.predict(X_test)\n",
    "y_pred[:10]\n",
    "y_test[:10]\n",
    "print(\"MSE test = \",MSE(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (b) MC model which includes item and previous item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18927\n",
      "18291 636\n",
      "18291 636\n"
     ]
    }
   ],
   "source": [
    "import dateutil\n",
    "# itemsPerUser[u], userIDs, itemIDs, interactionsWithPrevious, items, optimizer\n",
    "userIDs = {}\n",
    "itemIDs = {}\n",
    "interactions = []\n",
    "interactionsPerUser = defaultdict(list)\n",
    "\n",
    "for d in data:\n",
    "    u = d['user_id']\n",
    "    i = d['book_id']\n",
    "    t = d['date_added']\n",
    "    r = d['rating']\n",
    "    dt = dateutil.parser.parse(t)\n",
    "    t = int(dt.timestamp())\n",
    "    #print(t)\n",
    "    if userCount[u]<3 : continue\n",
    "    if not u in userIDs: userIDs[u] = len(userIDs)\n",
    "    if not i in itemIDs: itemIDs[i] = len(itemIDs)\n",
    "    interactions.append((t,u,i,r))\n",
    "    interactionsPerUser[u].append((t,i,r))    \n",
    "#interactions[0]\n",
    "interactions.sort()\n",
    "print(len(interactions))\n",
    "\n",
    "itemIDs['dummy'] = len(itemIDs)\n",
    "X_train = []\n",
    "X_test = []\n",
    "y_train = []\n",
    "y_test = []\n",
    "for u in interactionsPerUser:\n",
    "    interactionsPerUser[u].sort()\n",
    "    lastItem = 'dummy'\n",
    "    length = len(interactionsPerUser[u])\n",
    "    cnt = 0\n",
    "    for (t,i,r) in interactionsPerUser[u]:\n",
    "        cnt+=1\n",
    "        if cnt < length: \n",
    "            X_train.append((u,i,lastItem,r))\n",
    "            y_train.append(r)\n",
    "            lastItem = i\n",
    "        else :\n",
    "            X_test.append((u,i,lastItem))\n",
    "            y_test.append(r)\n",
    "print(len(X_train),len(X_test))\n",
    "print(len(y_train),len(y_test))\n",
    "\n",
    "itemsPerUser = defaultdict(set)\n",
    "for u,i,_,_ in X_train: #(u,i,lastItem,r)\n",
    "    itemsPerUser[u].add(i)\n",
    "items = list(itemIDs.keys())\n",
    "optimizer = tf.keras.optimizers.Adam(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MC(tf.keras.Model):\n",
    "    def __init__(self, K, lamb, IJ=1):\n",
    "        super(MC, self).__init__()\n",
    "        # Initialize variables\n",
    "        self.betaI = tf.Variable(tf.random.normal([len(itemIDs)],stddev=0.001))\n",
    "        self.gammaIJ = tf.Variable(tf.random.normal([len(itemIDs),K],stddev=0.001))\n",
    "        self.gammaJI = tf.Variable(tf.random.normal([len(itemIDs),K],stddev=0.001))\n",
    "        # Regularization coefficient\n",
    "        self.lamb = lamb\n",
    "        self.IJ = IJ\n",
    "\n",
    "    # Prediction for a single instance\n",
    "    def predict(self, i, j):\n",
    "        p = self.betaI[i] +  self.IJ * tf.tensordot(self.gammaIJ[i], self.gammaJI[j], 1)\n",
    "        return p\n",
    "\n",
    "    # Regularizer\n",
    "    def reg(self):\n",
    "        return self.lamb * (tf.nn.l2_loss(self.betaI) +\\\n",
    "                            tf.nn.l2_loss(self.gammaIJ) +\\\n",
    "                            tf.nn.l2_loss(self.gammaJI))\n",
    "\n",
    "    def call(self, sampleI, # item\n",
    "                   sampleJ, # previous item\n",
    "                   sampleK, sampleR): # negative item\n",
    "        i = tf.convert_to_tensor(sampleI, dtype=tf.int32)\n",
    "        j = tf.convert_to_tensor(sampleJ, dtype=tf.int32)\n",
    "        k = tf.convert_to_tensor(sampleK, dtype=tf.int32)\n",
    "        gamma_ij = tf.nn.embedding_lookup(self.gammaIJ, i)\n",
    "        gamma_ji = tf.nn.embedding_lookup(self.gammaJI, j)\n",
    "        beta_i = tf.nn.embedding_lookup(self.betaI, i)\n",
    "        x_ij = beta_i + self.IJ * tf.reduce_sum(tf.multiply(gamma_ij, gamma_ji), 1)\n",
    "        gamma_kj = tf.nn.embedding_lookup(self.gammaIJ, k)\n",
    "        gamma_jk = tf.nn.embedding_lookup(self.gammaJI, j)\n",
    "        beta_k = tf.nn.embedding_lookup(self.betaI, k)\n",
    "        x_kj = beta_k + self.IJ * tf.reduce_sum(tf.multiply(gamma_kj, gamma_jk), 1)\n",
    "        \n",
    "        mse = tf.keras.losses.MeanSquaredError()\n",
    "        return -tf.reduce_mean(tf.math.log(tf.math.sigmoid(x_ij - x_kj)))+ mse(x_ij,sampleR)\n",
    "    \n",
    "modelMC = MC(5, 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingStep(model, interactions):\n",
    "    with tf.GradientTape() as tape:\n",
    "        sampleI, sampleJ, sampleK, sampleR = [], [], [], []\n",
    "        for _ in range(100000):\n",
    "            u,i,j,r = random.choice(interactions) # positive sample\n",
    "            k = random.choice(items) # negative sample\n",
    "            while k in itemsPerUser[u]:\n",
    "                k = random.choice(items)\n",
    "            sampleI.append(itemIDs[i])\n",
    "            sampleJ.append(itemIDs[j])\n",
    "            sampleK.append(itemIDs[k])\n",
    "            sampleR.append(float(r))\n",
    "\n",
    "        loss = model(sampleI,sampleJ,sampleK,sampleR)\n",
    "        loss += model.reg()\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients((grad, var) for\n",
    "                              (grad, var) in zip(gradients, model.trainable_variables)\n",
    "                              if grad is not None)\n",
    "    return loss.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 10, objective = 4.13834\n",
      "iteration 20, objective = 1.4678717\n",
      "iteration 30, objective = 1.1231085\n",
      "iteration 40, objective = 0.7891793\n",
      "iteration 50, objective = 0.688743\n",
      "iteration 60, objective = 0.6427677\n",
      "iteration 70, objective = 0.6207298\n",
      "iteration 80, objective = 0.604971\n",
      "iteration 90, objective = 0.5938127\n",
      "iteration 100, objective = 0.5860394\n",
      "iteration 110, objective = 0.57835126\n",
      "iteration 120, objective = 0.5731423\n",
      "iteration 130, objective = 0.56896055\n",
      "iteration 140, objective = 0.5655522\n",
      "iteration 150, objective = 0.56131166\n",
      "iteration 160, objective = 0.55540156\n",
      "iteration 170, objective = 0.5564665\n",
      "iteration 180, objective = 0.5507009\n",
      "iteration 190, objective = 0.5519376\n",
      "iteration 200, objective = 0.550581\n",
      "iteration 210, objective = 0.55066055\n",
      "iteration 220, objective = 0.54869735\n",
      "iteration 230, objective = 0.54819083\n",
      "iteration 240, objective = 0.54728955\n",
      "iteration 250, objective = 0.54727554\n",
      "iteration 260, objective = 0.54471296\n",
      "iteration 270, objective = 0.544299\n",
      "iteration 280, objective = 0.54173803\n",
      "iteration 290, objective = 0.54505545\n",
      "iteration 300, objective = 0.54363304\n",
      "iteration 310, objective = 0.5427178\n",
      "iteration 320, objective = 0.54215485\n",
      "iteration 330, objective = 0.54277897\n",
      "iteration 340, objective = 0.5393973\n",
      "iteration 350, objective = 0.5434398\n",
      "iteration 360, objective = 0.5427398\n",
      "iteration 370, objective = 0.54314435\n",
      "iteration 380, objective = 0.53990793\n",
      "iteration 390, objective = 0.54345894\n",
      "iteration 400, objective = 0.5412914\n",
      "iteration 410, objective = 0.5371492\n",
      "iteration 420, objective = 0.5400574\n",
      "iteration 430, objective = 0.5378474\n",
      "iteration 440, objective = 0.53764486\n",
      "iteration 450, objective = 0.53639424\n",
      "iteration 460, objective = 0.5378227\n",
      "iteration 470, objective = 0.53672457\n",
      "iteration 480, objective = 0.53929245\n",
      "iteration 490, objective = 0.5363729\n",
      "iteration 500, objective = 0.5380682\n",
      "iteration 510, objective = 0.5389014\n",
      "iteration 520, objective = 0.5386449\n",
      "iteration 530, objective = 0.53421676\n",
      "iteration 540, objective = 0.53760684\n",
      "iteration 550, objective = 0.5375755\n",
      "iteration 560, objective = 0.5361507\n",
      "iteration 570, objective = 0.5370755\n",
      "iteration 580, objective = 0.53628033\n",
      "iteration 590, objective = 0.53795755\n",
      "iteration 600, objective = 0.5344045\n"
     ]
    }
   ],
   "source": [
    "for i in range(600):\n",
    "    obj = trainingStep(modelMC, X_train)\n",
    "    if (i % 10 == 9): print(\"iteration \" + str(i+1) + \", objective = \" + str(obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 10, objective = 0.5326002\n",
      "iteration 20, objective = 0.5337547\n",
      "iteration 30, objective = 0.5348592\n",
      "iteration 40, objective = 0.5333945\n",
      "iteration 50, objective = 0.5337779\n",
      "iteration 60, objective = 0.53178316\n",
      "iteration 70, objective = 0.53358996\n",
      "iteration 80, objective = 0.5333319\n",
      "iteration 90, objective = 0.53025126\n",
      "iteration 100, objective = 0.5328586\n",
      "iteration 110, objective = 0.5327846\n",
      "iteration 120, objective = 0.5334447\n",
      "iteration 130, objective = 0.5328021\n",
      "iteration 140, objective = 0.5310075\n",
      "iteration 150, objective = 0.5327859\n",
      "iteration 160, objective = 0.53231776\n",
      "iteration 170, objective = 0.5324224\n",
      "iteration 180, objective = 0.5300671\n",
      "iteration 190, objective = 0.5288187\n",
      "iteration 200, objective = 0.53194344\n",
      "iteration 210, objective = 0.5303365\n",
      "iteration 220, objective = 0.53093415\n",
      "iteration 230, objective = 0.53158236\n",
      "iteration 240, objective = 0.5297749\n",
      "iteration 250, objective = 0.53151655\n",
      "iteration 260, objective = 0.5314349\n",
      "iteration 270, objective = 0.5277928\n",
      "iteration 280, objective = 0.5314529\n",
      "iteration 290, objective = 0.53008497\n",
      "iteration 300, objective = 0.52824694\n",
      "iteration 310, objective = 0.5267406\n",
      "iteration 320, objective = 0.53021646\n",
      "iteration 330, objective = 0.52800006\n",
      "iteration 340, objective = 0.5272641\n",
      "iteration 350, objective = 0.52776515\n",
      "iteration 360, objective = 0.5294466\n",
      "iteration 370, objective = 0.5290521\n",
      "iteration 380, objective = 0.52797383\n",
      "iteration 390, objective = 0.5289345\n",
      "iteration 400, objective = 0.52735424\n",
      "iteration 410, objective = 0.5270269\n",
      "iteration 420, objective = 0.5279036\n",
      "iteration 430, objective = 0.5276952\n",
      "iteration 440, objective = 0.5276272\n",
      "iteration 450, objective = 0.52910197\n",
      "iteration 460, objective = 0.52851444\n",
      "iteration 470, objective = 0.5264541\n",
      "iteration 480, objective = 0.5295156\n",
      "iteration 490, objective = 0.5272901\n",
      "iteration 500, objective = 0.5262562\n",
      "iteration 510, objective = 0.52695566\n",
      "iteration 520, objective = 0.5266715\n",
      "iteration 530, objective = 0.5270011\n",
      "iteration 540, objective = 0.5261657\n",
      "iteration 550, objective = 0.5250126\n",
      "iteration 560, objective = 0.5241653\n",
      "iteration 570, objective = 0.5274565\n",
      "iteration 580, objective = 0.5269251\n",
      "iteration 590, objective = 0.5270935\n",
      "iteration 600, objective = 0.527153\n"
     ]
    }
   ],
   "source": [
    "for i in range(600):\n",
    "    obj = trainingStep(modelMC, X_train)\n",
    "    if (i % 10 == 9): print(\"iteration \" + str(i+1) + \", objective = \" + str(obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE train =  0.04514250267013937\n",
      "MSE test =  13.737801945380893\n"
     ]
    }
   ],
   "source": [
    "def MSE(predictions, labels):\n",
    "    differences = [(x-y)**2 for x,y in zip(predictions,labels)]\n",
    "    return sum(differences) / len(differences)\n",
    "\n",
    "pred_train = []\n",
    "for u,i,j,_ in X_train:\n",
    "    pred = modelMC.predict(itemIDs[i],itemIDs[j]).numpy()\n",
    "    pred_train.append(pred)\n",
    "print(\"MSE train = \",MSE(pred_train, y_train))\n",
    "\n",
    "pred_test = []\n",
    "for u,i,j in X_test:\n",
    "    pred = modelMC.predict(itemIDs[i],itemIDs[j]).numpy()\n",
    "    pred_test.append(pred)\n",
    "print(\"MSE test = \",MSE(pred_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (c) FPMC model which includes user, item and previous item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18927\n",
      "18291 636\n",
      "18291 636\n"
     ]
    }
   ],
   "source": [
    "import dateutil\n",
    "# itemsPerUser[u], userIDs, itemIDs, interactionsWithPrevious, items, optimizer\n",
    "userIDs = {}\n",
    "itemIDs = {}\n",
    "interactions = []\n",
    "interactionsPerUser = defaultdict(list)\n",
    "\n",
    "for d in data:\n",
    "    u = d['user_id']\n",
    "    i = d['book_id']\n",
    "    t = d['date_added']\n",
    "    r = d['rating']\n",
    "    dt = dateutil.parser.parse(t)\n",
    "    t = int(dt.timestamp())\n",
    "    #print(t)\n",
    "    if userCount[u]<3 : continue\n",
    "    if not u in userIDs: userIDs[u] = len(userIDs)\n",
    "    if not i in itemIDs: itemIDs[i] = len(itemIDs)\n",
    "    interactions.append((t,u,i,r))\n",
    "    interactionsPerUser[u].append((t,i,r))    \n",
    "#interactions[0]\n",
    "interactions.sort()\n",
    "print(len(interactions))\n",
    "\n",
    "itemIDs['dummy'] = len(itemIDs)\n",
    "X_train = []\n",
    "X_test = []\n",
    "y_train = []\n",
    "y_test = []\n",
    "for u in interactionsPerUser:\n",
    "    interactionsPerUser[u].sort()\n",
    "    lastItem = 'dummy'\n",
    "    length = len(interactionsPerUser[u])\n",
    "    cnt = 0\n",
    "    for (t,i,r) in interactionsPerUser[u]:\n",
    "        cnt+=1\n",
    "        if cnt < length: \n",
    "            X_train.append((u,i,lastItem,r))\n",
    "            y_train.append(r)\n",
    "            lastItem = i\n",
    "        else :\n",
    "            X_test.append((u,i,lastItem))\n",
    "            y_test.append(r)\n",
    "print(len(X_train),len(X_test))\n",
    "print(len(y_train),len(y_test))\n",
    "\n",
    "itemsPerUser = defaultdict(set)\n",
    "for u,i,_,_ in X_train: #(u,i,lastItem,r)\n",
    "    itemsPerUser[u].add(i)\n",
    "items = list(itemIDs.keys())\n",
    "optimizer = tf.keras.optimizers.Adam(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FPMC(tf.keras.Model):\n",
    "    def __init__(self, K, lamb, UI = 1, IJ = 1):\n",
    "        super(FPMC, self).__init__()\n",
    "        # Initialize variables\n",
    "        self.betaU = tf.Variable(tf.random.normal([len(userIDs)],stddev=0.001))\n",
    "        self.betaI = tf.Variable(tf.random.normal([len(itemIDs)],stddev=0.001))\n",
    "        self.gammaUI = tf.Variable(tf.random.normal([len(userIDs),K],stddev=0.001))\n",
    "        self.gammaIU = tf.Variable(tf.random.normal([len(itemIDs),K],stddev=0.001))\n",
    "        self.gammaIJ = tf.Variable(tf.random.normal([len(itemIDs),K],stddev=0.001))\n",
    "        self.gammaJI = tf.Variable(tf.random.normal([len(itemIDs),K],stddev=0.001))\n",
    "        # Regularization coefficient\n",
    "        self.lamb = lamb\n",
    "        # Which terms to include\n",
    "        self.UI = UI\n",
    "        self.IJ = IJ\n",
    "\n",
    "    # Prediction for a single instance\n",
    "    def predict(self, u, i, j):\n",
    "        p = self.betaU[u]+self.betaI[i] + self.UI * tf.tensordot(self.gammaUI[u], self.gammaIU[i], 1) +\\\n",
    "                            self.IJ * tf.tensordot(self.gammaIJ[i], self.gammaJI[j], 1)\n",
    "        return p\n",
    "\n",
    "    # Regularizer\n",
    "    def reg(self):\n",
    "        return self.lamb * (tf.nn.l2_loss(self.betaU) +\\\n",
    "                            tf.nn.l2_loss(self.betaI) +\\\n",
    "                            tf.nn.l2_loss(self.gammaUI) +\\\n",
    "                            tf.nn.l2_loss(self.gammaIU) +\\\n",
    "                            tf.nn.l2_loss(self.gammaIJ) +\\\n",
    "                            tf.nn.l2_loss(self.gammaJI))\n",
    "\n",
    "    def call(self, sampleU, # user\n",
    "                   sampleI, # item\n",
    "                   sampleJ, # previous item\n",
    "                   sampleK, sampleR): # negative item\n",
    "        u = tf.convert_to_tensor(sampleU, dtype=tf.int32)\n",
    "        i = tf.convert_to_tensor(sampleI, dtype=tf.int32)\n",
    "        j = tf.convert_to_tensor(sampleJ, dtype=tf.int32)\n",
    "        k = tf.convert_to_tensor(sampleK, dtype=tf.int32)\n",
    "        gamma_ui = tf.nn.embedding_lookup(self.gammaUI, u)\n",
    "        gamma_iu = tf.nn.embedding_lookup(self.gammaIU, i)\n",
    "        gamma_ij = tf.nn.embedding_lookup(self.gammaIJ, i)\n",
    "        gamma_ji = tf.nn.embedding_lookup(self.gammaJI, j)\n",
    "        beta_u = tf.nn.embedding_lookup(self.betaU, u)\n",
    "        beta_i = tf.nn.embedding_lookup(self.betaI, i)\n",
    "        x_uij = beta_u + beta_i + self.UI * tf.reduce_sum(tf.multiply(gamma_ui, gamma_iu), 1) +\\\n",
    "                         self.IJ * tf.reduce_sum(tf.multiply(gamma_ij, gamma_ji), 1)\n",
    "        gamma_uk = tf.nn.embedding_lookup(self.gammaUI, u)\n",
    "        gamma_ku = tf.nn.embedding_lookup(self.gammaIU, k)\n",
    "        gamma_kj = tf.nn.embedding_lookup(self.gammaIJ, k)\n",
    "        gamma_jk = tf.nn.embedding_lookup(self.gammaJI, j)\n",
    "        beta_u = tf.nn.embedding_lookup(self.betaU, u)\n",
    "        beta_k = tf.nn.embedding_lookup(self.betaI, k)\n",
    "        x_ukj = beta_u + beta_k + self.UI * tf.reduce_sum(tf.multiply(gamma_uk, gamma_ku), 1) +\\\n",
    "                         self.IJ * tf.reduce_sum(tf.multiply(gamma_kj, gamma_jk), 1)\n",
    "        \n",
    "        mse = tf.keras.losses.MeanSquaredError()\n",
    "        return -tf.reduce_mean(tf.math.log(tf.math.sigmoid(x_uij - x_ukj)))+ mse(x_uij,sampleR)\n",
    "    \n",
    "modelFPMC = FPMC(5, 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingStep(model, interactions):\n",
    "    with tf.GradientTape() as tape:\n",
    "        sampleU, sampleI, sampleJ, sampleK, sampleR = [], [], [], [], []\n",
    "        for _ in range(100000):\n",
    "            u,i,j,r = random.choice(interactions) # positive sample\n",
    "            k = random.choice(items) # negative sample\n",
    "            while k in itemsPerUser[u]:\n",
    "                k = random.choice(items)\n",
    "            sampleU.append(userIDs[u])\n",
    "            sampleI.append(itemIDs[i])\n",
    "            sampleJ.append(itemIDs[j])\n",
    "            sampleK.append(itemIDs[k])\n",
    "            sampleR.append(float(r))\n",
    "\n",
    "        loss = model(sampleU,sampleI,sampleJ,sampleK,sampleR)\n",
    "        loss += model.reg()\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients((grad, var) for\n",
    "                              (grad, var) in zip(gradients, model.trainable_variables)\n",
    "                              if grad is not None)\n",
    "    return loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 10, objective = 1.5656526\n",
      "iteration 20, objective = 1.2348145\n",
      "iteration 30, objective = 0.895774\n",
      "iteration 40, objective = 0.65240955\n",
      "iteration 50, objective = 0.50799006\n",
      "iteration 60, objective = 0.42495096\n",
      "iteration 70, objective = 0.37038845\n",
      "iteration 80, objective = 0.33357084\n",
      "iteration 90, objective = 0.3100459\n",
      "iteration 100, objective = 0.29241621\n",
      "iteration 110, objective = 0.27959573\n",
      "iteration 120, objective = 0.26924568\n",
      "iteration 130, objective = 0.26147848\n",
      "iteration 140, objective = 0.25636426\n",
      "iteration 150, objective = 0.25175643\n",
      "iteration 160, objective = 0.24752866\n",
      "iteration 170, objective = 0.24323055\n",
      "iteration 180, objective = 0.2398307\n",
      "iteration 190, objective = 0.23606\n",
      "iteration 200, objective = 0.23436159\n",
      "iteration 210, objective = 0.23050463\n",
      "iteration 220, objective = 0.22710988\n",
      "iteration 230, objective = 0.22681578\n",
      "iteration 240, objective = 0.22320634\n",
      "iteration 250, objective = 0.22324425\n",
      "iteration 260, objective = 0.21927588\n",
      "iteration 270, objective = 0.21816015\n",
      "iteration 280, objective = 0.21729755\n",
      "iteration 290, objective = 0.21482633\n",
      "iteration 300, objective = 0.2130304\n"
     ]
    }
   ],
   "source": [
    "for i in range(300):\n",
    "    obj = trainingStep(modelFPMC, X_train)\n",
    "    if (i % 10 == 9): print(\"iteration \" + str(i+1) + \", objective = \" + str(obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 10, objective = 0.21171278\n",
      "iteration 20, objective = 0.21088418\n",
      "iteration 30, objective = 0.20812409\n",
      "iteration 40, objective = 0.20991042\n",
      "iteration 50, objective = 0.20626906\n",
      "iteration 60, objective = 0.20595244\n",
      "iteration 70, objective = 0.20624606\n",
      "iteration 80, objective = 0.20444506\n",
      "iteration 90, objective = 0.20334204\n",
      "iteration 100, objective = 0.2020027\n",
      "iteration 110, objective = 0.20169473\n",
      "iteration 120, objective = 0.20018199\n",
      "iteration 130, objective = 0.20017701\n",
      "iteration 140, objective = 0.19955021\n",
      "iteration 150, objective = 0.19904074\n",
      "iteration 160, objective = 0.19734436\n",
      "iteration 170, objective = 0.19762924\n",
      "iteration 180, objective = 0.19678152\n",
      "iteration 190, objective = 0.19634858\n",
      "iteration 200, objective = 0.19579735\n",
      "iteration 210, objective = 0.19402504\n",
      "iteration 220, objective = 0.19615518\n",
      "iteration 230, objective = 0.1951198\n",
      "iteration 240, objective = 0.19289783\n",
      "iteration 250, objective = 0.1921893\n",
      "iteration 260, objective = 0.19239932\n",
      "iteration 270, objective = 0.19240528\n",
      "iteration 280, objective = 0.1937513\n",
      "iteration 290, objective = 0.19290987\n",
      "iteration 300, objective = 0.19064425\n"
     ]
    }
   ],
   "source": [
    "for i in range(300):\n",
    "    obj = trainingStep(modelFPMC, X_train)\n",
    "    if (i % 10 == 9): print(\"iteration \" + str(i+1) + \", objective = \" + str(obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE train =  0.020084221563801165\n",
      "MSE test =  10.812561900693636\n"
     ]
    }
   ],
   "source": [
    "def MSE(predictions, labels):\n",
    "    differences = [(x-y)**2 for x,y in zip(predictions,labels)]\n",
    "    return sum(differences) / len(differences)\n",
    "\n",
    "pred_train = []\n",
    "for u,i,j,_ in X_train:\n",
    "    pred = modelFPMC.predict(userIDs[u],itemIDs[i],itemIDs[j]).numpy()\n",
    "    pred_train.append(pred)\n",
    "print(\"MSE train = \",MSE(pred_train, y_train))\n",
    "\n",
    "pred_test = []\n",
    "for u,i,j in X_test:\n",
    "    pred = modelFPMC.predict(userIDs[u],itemIDs[i],itemIDs[j]).numpy()\n",
    "    pred_test.append(pred)\n",
    "print(\"MSE test = \",MSE(pred_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 10, objective = 0.19159523\n",
      "iteration 20, objective = 0.1904184\n",
      "iteration 30, objective = 0.19104567\n",
      "iteration 40, objective = 0.19058432\n",
      "iteration 50, objective = 0.18850201\n",
      "iteration 60, objective = 0.18892846\n",
      "iteration 70, objective = 0.18817797\n",
      "iteration 80, objective = 0.18905973\n",
      "iteration 90, objective = 0.18695144\n",
      "iteration 100, objective = 0.18653814\n",
      "iteration 110, objective = 0.18768285\n",
      "iteration 120, objective = 0.18654492\n",
      "iteration 130, objective = 0.18747246\n",
      "iteration 140, objective = 0.18606657\n",
      "iteration 150, objective = 0.1880572\n",
      "iteration 160, objective = 0.18655753\n",
      "iteration 170, objective = 0.18653783\n",
      "iteration 180, objective = 0.18628897\n",
      "iteration 190, objective = 0.18616599\n",
      "iteration 200, objective = 0.18638262\n",
      "iteration 210, objective = 0.18388262\n",
      "iteration 220, objective = 0.18572797\n",
      "iteration 230, objective = 0.18442294\n",
      "iteration 240, objective = 0.18305612\n",
      "iteration 250, objective = 0.18171448\n",
      "iteration 260, objective = 0.1820064\n",
      "iteration 270, objective = 0.18189988\n",
      "iteration 280, objective = 0.18252403\n",
      "iteration 290, objective = 0.18088749\n",
      "iteration 300, objective = 0.18198305\n"
     ]
    }
   ],
   "source": [
    "for i in range(300):\n",
    "    obj = trainingStep(modelFPMC, X_train)\n",
    "    if (i % 10 == 9): print(\"iteration \" + str(i+1) + \", objective = \" + str(obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE train =  0.019621448033968612\n",
      "MSE test =  11.231530281475283\n"
     ]
    }
   ],
   "source": [
    "def MSE(predictions, labels):\n",
    "    differences = [(x-y)**2 for x,y in zip(predictions,labels)]\n",
    "    return sum(differences) / len(differences)\n",
    "\n",
    "pred_train = []\n",
    "for u,i,j,_ in X_train:\n",
    "    pred = modelFPMC.predict(userIDs[u],itemIDs[i],itemIDs[j]).numpy()\n",
    "    pred_train.append(pred)\n",
    "print(\"MSE train = \",MSE(pred_train, y_train))\n",
    "\n",
    "pred_test = []\n",
    "for u,i,j in X_test:\n",
    "    pred = modelFPMC.predict(userIDs[u],itemIDs[i],itemIDs[j]).numpy()\n",
    "    pred_test.append(pred)\n",
    "print(\"MSE test = \",MSE(pred_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans for Problem3 :\n",
    "\n",
    "a. regular latent factor model:\n",
    "\n",
    "    MSE train =  0.013160128282292967\n",
    "    MSE test =  1.3137676309457482\n",
    "    \n",
    "b. non-personalized Markov Chain model with previous item and current item:\n",
    "\n",
    "    MSE train =  0.04514250267013937\n",
    "    MSE test =  13.737801945380893\n",
    "\n",
    "c. Markov Chain model with user, previous item and current item: \n",
    "\n",
    "    MSE train =  0.019621448033968612\n",
    "    MSE test =  11.231530281475283"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"problem4.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans for Problem4 :\n",
    "\n",
    "a. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
