{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"hw3.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"hw3.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from collections import defaultdict\n",
    "from sklearn import linear_model\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readGz(path):\n",
    "    for l in gzip.open(path, 'rt'):\n",
    "        yield eval(l)\n",
    "\n",
    "def readCSV(path):\n",
    "    f = gzip.open(path, 'rt')\n",
    "    c = csv.reader(f)\n",
    "    header = next(c)\n",
    "    #print(header)\n",
    "    for l in c:\n",
    "        d = dict(zip(header,l))\n",
    "        yield d['user_id'],d['recipe_id'],d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"problem1.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"problem1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "user_per_recipe = defaultdict(set)\n",
    "recipe_per_user = defaultdict(set)\n",
    "\n",
    "for user,recipe,d in readCSV(\"trainInteractions.csv.gz\"):\n",
    "    data.append([user,recipe])\n",
    "    recipe_per_user[user].add(recipe)\n",
    "    user_per_recipe[recipe].add(user)\n",
    "\n",
    "train = data[:400000]\n",
    "valid = data[400000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "print(len(valid))\n",
    "#print(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "20000\n",
      "40000\n",
      "60000\n",
      "80000\n",
      "200000\n"
     ]
    }
   ],
   "source": [
    "import random #this block takes so long to run\n",
    "recipe_set = set(recipe_per_user.keys())\n",
    "cnt = 0\n",
    "valid_add = []\n",
    "for user,recipe in valid :\n",
    "    #print(user, recipe)\n",
    "    if cnt%20000==0:\n",
    "        print(cnt)\n",
    "    cnt+=1\n",
    "    false_set = recipe_set - recipe_per_user[user]\n",
    "    recipe_false = random.sample(list(false_set), 1)\n",
    "    #print(recipe_false)\n",
    "    valid_add.append([user, recipe_false[0]])\n",
    "valid += valid_add\n",
    "print(len(valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "20000\n",
      "40000\n",
      "60000\n",
      "80000\n",
      "100000\n",
      "120000\n",
      "140000\n",
      "160000\n",
      "180000\n",
      "0.69627\n"
     ]
    }
   ],
   "source": [
    "### Would-cook baseline: just rank which recipes are popular and which are not, and return '1' if a recipe is among the top-ranked\n",
    "\n",
    "recipeCount = defaultdict(int)\n",
    "totalCooked = 0\n",
    "\n",
    "for user,recipe,_ in readCSV(\"trainInteractions.csv.gz\"):\n",
    "  recipeCount[recipe] += 1\n",
    "  totalCooked += 1\n",
    "\n",
    "mostPopular = [(recipeCount[x], x) for x in recipeCount]\n",
    "mostPopular.sort()\n",
    "mostPopular.reverse()\n",
    "# high to low\n",
    "#print(mostPopular)\n",
    "\n",
    "return1 = set()\n",
    "count = 0\n",
    "for ic, i in mostPopular:\n",
    "  count += ic\n",
    "  return1.add(i)\n",
    "  if count > totalCooked/2: break\n",
    "\n",
    "y_pred = []\n",
    "cnt=0\n",
    "for user,recipe in valid :\n",
    "    #print(recipe)\n",
    "    if cnt%20000==0:\n",
    "        print(cnt)\n",
    "    cnt+=1\n",
    "    if recipe in return1:\n",
    "        y_pred.append(1)\n",
    "    else :\n",
    "        y_pred.append(0)\n",
    "        \n",
    "y_valid = [1]*100000+[0]*100000\n",
    "\n",
    "TP = sum([(p and l) for (p,l) in zip(y_pred, y_valid)])\n",
    "FP = sum([(p and not l) for (p,l) in zip(y_pred, y_valid)])\n",
    "TN = sum([(not p and not l) for (p,l) in zip(y_pred, y_valid)])\n",
    "FN = sum([(not p and l) for (p,l) in zip(y_pred, y_valid)])\n",
    "ACC = (TP + TN) / (TP + FP + TN + FN)\n",
    "print(ACC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans for Problem1 :\n",
    "\n",
    "a. accuracy of the baseline model on validation set = 0.696805"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"problem2.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"problem2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentile :  10 , ACC =  0.548565\n",
      "percentile :  20 , ACC =  0.59391\n",
      "percentile :  30 , ACC =  0.634985\n",
      "percentile :  40 , ACC =  0.66807\n",
      "percentile :  50 , ACC =  0.69627\n",
      "percentile :  60 , ACC =  0.723925\n",
      "percentile :  70 , ACC =  0.754945\n",
      "percentile :  80 , ACC =  0.783615\n",
      "percentile :  90 , ACC =  0.8065\n"
     ]
    }
   ],
   "source": [
    "def predict_by_threshold(percentile):\n",
    "    return1 = set()\n",
    "    count = 0\n",
    "    for ic, i in mostPopular:\n",
    "      count += ic\n",
    "      return1.add(i)\n",
    "      if count > totalCooked*percentile/100: break\n",
    "    y_pred = []\n",
    "    for u,i in valid :\n",
    "        if i in return1:\n",
    "            y_pred.append(1)\n",
    "        else :\n",
    "            y_pred.append(0)\n",
    "    TP = sum([(p and l) for (p,l) in zip(y_pred, y_valid)])\n",
    "    FP = sum([(p and not l) for (p,l) in zip(y_pred, y_valid)])\n",
    "    TN = sum([(not p and not l) for (p,l) in zip(y_pred, y_valid)])\n",
    "    FN = sum([(not p and l) for (p,l) in zip(y_pred, y_valid)])\n",
    "    ACC = (TP + TN) / (TP + FP + TN + FN)\n",
    "    print(\"percentile : \",percentile,\", ACC = \",ACC)\n",
    "    \n",
    "for percentile in range(10,100,10):\n",
    "    predict_by_threshold(percentile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans for Problem2 :\n",
    "\n",
    "a. accuracy of the best threshold model on validation set = 0.80843\n",
    "   with threshold = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"problem3.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"problem3.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold :  10 , ACC =  0.8077\n",
      "threshold :  20 , ACC =  0.75093\n",
      "threshold :  30 , ACC =  0.70707\n",
      "threshold :  40 , ACC =  0.655665\n",
      "threshold :  50 , ACC =  0.595455\n",
      "threshold :  60 , ACC =  0.59426\n",
      "threshold :  70 , ACC =  0.58028\n",
      "threshold :  80 , ACC =  0.57944\n",
      "threshold :  90 , ACC =  0.57944\n"
     ]
    }
   ],
   "source": [
    "# Jaccard similarity interchanging users and items\n",
    "def Jaccard(s1, s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1.union(s2))\n",
    "    if denom == 0:\n",
    "        return 0\n",
    "    return numer / denom\n",
    "\n",
    "# prediction based on Jaccard Similarity\n",
    "def predict_by_jaccard(threshold):\n",
    "    y_pred = []\n",
    "    y_valid = [1]*100000+[0]*100000\n",
    "    for u,i in valid :\n",
    "        items = recipe_per_user[u]\n",
    "        s1 = user_per_recipe[i]\n",
    "        jaccard_sim = [0]\n",
    "        for g in items-{i} :\n",
    "            s2 = user_per_recipe[g]\n",
    "            jaccard_sim.append(Jaccard(s1, s2))\n",
    "        #print(jaccard_sim)\n",
    "        if max(jaccard_sim)> threshold/100:\n",
    "            y_pred.append(1)\n",
    "        else:\n",
    "            y_pred.append(0)\n",
    "            \n",
    "    TP = sum([(p and l) for (p,l) in zip(y_pred, y_valid)])\n",
    "    FP = sum([(p and not l) for (p,l) in zip(y_pred, y_valid)])\n",
    "    TN = sum([(not p and not l) for (p,l) in zip(y_pred, y_valid)])\n",
    "    FN = sum([(not p and l) for (p,l) in zip(y_pred, y_valid)])\n",
    "    ACC = (TP + TN) / (TP + FP + TN + FN)\n",
    "    print(\"threshold : \",threshold,\", ACC = \",ACC)\n",
    "\n",
    "for threshold in range(10,100,10):\n",
    "    predict_by_jaccard(threshold)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans for Problem3 :\n",
    "\n",
    "a. accuracy of the best jaccard model on validation set = 0.71121\n",
    "   with threshold = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"problem4.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"problem4.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_by_combine(percentile, threshold):\n",
    "    return1 = set()\n",
    "    count = 0\n",
    "    for ic, i in mostPopular:\n",
    "      count += ic\n",
    "      return1.add(i)\n",
    "      if count > totalCooked*percentile/100: break\n",
    "    y_pred = []\n",
    "    for u,i in valid :\n",
    "        if i in return1:\n",
    "            y_pred.append(1)\n",
    "        else :\n",
    "            y_pred.append(0)\n",
    "            \n",
    "    TP = sum([(p and l) for (p,l) in zip(y_pred, y_valid)])\n",
    "    FP = sum([(p and not l) for (p,l) in zip(y_pred, y_valid)])\n",
    "    TN = sum([(not p and not l) for (p,l) in zip(y_pred, y_valid)])\n",
    "    FN = sum([(not p and l) for (p,l) in zip(y_pred, y_valid)])\n",
    "    ACC = (TP + TN) / (TP + FP + TN + FN)\n",
    "    print(\"percentile : \",percentile,\", ACC = \",ACC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans for Problem4 :\n",
    "\n",
    "a. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"problem5.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"problem5.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model on train + validation and predict\n",
    "def write_prediction_most_popular(threshold):\n",
    "    predictions = open(\"predictions_Made_popular_%d.txt\" % threshold,'w')\n",
    "    return1 = set()\n",
    "    count = 0\n",
    "    for ic, i in mostPopular:\n",
    "        count += ic\n",
    "        return1.add(i)\n",
    "        if count > totalCooked*threshold/100: break\n",
    "\n",
    "    for l in open(\"stub_Made.txt\"):\n",
    "      if l.startswith(\"user_id\"):\n",
    "        #header\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "      u,i = l.strip().split('-')\n",
    "      if i in return1:\n",
    "        predictions.write(u + '-' + i + \",1\\n\")\n",
    "      else:\n",
    "        predictions.write(u + '-' + i + \",0\\n\")\n",
    "\n",
    "    predictions.close()\n",
    "\n",
    "for threshold in [50,60,70,80,90]:\n",
    "    write_prediction_most_popular(threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model on train + validation and predict\n",
    "def write_prediction_jaccard(threshold):\n",
    "    predictions = open(\"predictions_Made_jaccard_%d.txt\"%threshold, 'w')\n",
    "\n",
    "    for l in open(\"stub_Made.txt\"):\n",
    "      if l.startswith(\"user_id\"):\n",
    "        #header\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "      u,i = l.strip().split('-')\n",
    "      items = recipe_per_user[u]\n",
    "      s1 = user_per_recipe[i]\n",
    "      jaccard_sim = [0]\n",
    "      for g in items-{i} :\n",
    "            s2 = user_per_recipe[g]\n",
    "            jaccard_sim.append(Jaccard(s1, s2))\n",
    "      #print(jaccard_sim)\n",
    "      if max(jaccard_sim)> threshold/100:\n",
    "            predictions.write(u + '-' + i + \",1\\n\")\n",
    "      else:\n",
    "            predictions.write(u + '-' + i + \",0\\n\")  \n",
    "\n",
    "    predictions.close()\n",
    "\n",
    "for threshold in [10,20]:\n",
    "    write_prediction_jaccard(threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans for Problem5 :\n",
    "\n",
    "a. accuracy of the most popular 50 model on validation set = 0.68660\n",
    "   accuracy of the most popular 60 model on validation set = 0.69930\n",
    "   accuracy of the most popular 70 model on validation set = 0.68480\n",
    "   accuracy of the most popular 80 model on validation set = 0.70230\n",
    "   accuracy of the most popular 90 model on validation set = 0.63180\n",
    "   accuracy of the jaccard threshold 10 model on validation set = 0.52500\n",
    "   accuracy of the jaccard threshold 20 model on validation set = 0.50849"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"problem9.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"problem9.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_data = []\n",
    "user_per_recipe = defaultdict(set)\n",
    "recipe_per_user = defaultdict(set)\n",
    "\n",
    "for user,recipe,d in readCSV(\"trainInteractions.csv.gz\"):\n",
    "    rating_data.append([user,recipe,int(d['rating'])])\n",
    "    recipe_per_user[user].add(recipe)\n",
    "    user_per_recipe[recipe].add(user)\n",
    "\n",
    "rating_train = rating_data[:400000]\n",
    "rating_valid = rating_data[400000:]\n",
    "\n",
    "N = len(rating_data)\n",
    "nUsers = len(list(recipe_per_user.keys()))\n",
    "nItems = len(list(user_per_recipe.keys()))\n",
    "users = list(recipe_per_user.keys())\n",
    "items = list(user_per_recipe.keys())\n",
    "#print(nUsers, nItems)\n",
    "#print(len(users),len(items))\n",
    "#print(len(users)+len(items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.580794\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "alpha = np.mean(np.array([d[2] for d in rating_data]))\n",
    "print(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "userBiases = defaultdict(float)\n",
    "itemBiases = defaultdict(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(user, item):\n",
    "    return alpha + userBiases[user] + itemBiases[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack(theta):\n",
    "    #print(\"unpack, len(theta) = \", len(theta))\n",
    "    global alpha\n",
    "    global userBiases\n",
    "    global itemBiases\n",
    "    alpha = theta[0]\n",
    "    #print(users)\n",
    "    #print(items)\n",
    "    userBiases = dict(zip(users, theta[1:nUsers+1]))\n",
    "    itemBiases = dict(zip(items, theta[1+nUsers:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(predictions, labels):\n",
    "    differences = [(x-y)**2 for x,y in zip(predictions,labels)]\n",
    "    return sum(differences) / len(differences)\n",
    "\n",
    "def cost(theta, labels, lamb):\n",
    "    unpack(theta)\n",
    "    #print(\"cost, len(theta) = \", len(theta))\n",
    "    predictions = [prediction(d[0], d[1]) for d in rating_data]\n",
    "    cost = MSE(predictions, labels)\n",
    "    print(\"MSE = \" + str(cost))\n",
    "    for u in userBiases:\n",
    "        cost += lamb*userBiases[u]**2\n",
    "    for i in itemBiases:\n",
    "        cost += lamb*itemBiases[i]**2\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(theta, labels, lamb):\n",
    "    unpack(theta)\n",
    "    N = len(rating_data)\n",
    "    dalpha = 0\n",
    "    dUserBiases = defaultdict(float)\n",
    "    dItemBiases = defaultdict(float)\n",
    "    for d in rating_data:\n",
    "        u,i = d[0], d[1]\n",
    "        pred = prediction(u, i)\n",
    "        diff = pred - d[2]\n",
    "        dalpha += 2/N*diff\n",
    "        dUserBiases[u] += 2/N*diff\n",
    "        dItemBiases[i] += 2/N*diff\n",
    "    for u in userBiases:\n",
    "        dUserBiases[u] += 2*lamb*userBiases[u]\n",
    "    for i in itemBiases:\n",
    "        dItemBiases[i] += 2*lamb*itemBiases[i]\n",
    "    dtheta = [dalpha] + [dUserBiases[u] for u in users] + [dItemBiases[i] for i in items]\n",
    "    #print(\"len [dUserBiases[u] for u in users] = \",len([dUserBiases[u] for u in users])) #13533\n",
    "    #print(\"len [dItemBiases[i] for i in items] = \",len([dItemBiases[i] for i in items])) #163899 ???\n",
    "    #print(\"len(items) = \",len(items)) \n",
    "    #print(\"derivative, len(dtheta) = \", len(dtheta))\n",
    "    return np.hstack((dalpha,[dUserBiases[u] for u in users],[dItemBiases[i] for i in items]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 0.9008923295603781\n",
      "MSE = 0.8880111212499086\n",
      "MSE = 0.90075870005668\n",
      "MSE = 0.9007586515538737\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 4.58067116e+00, -4.78253046e-05, -6.45546452e-06, ...,\n",
       "         8.38808750e-07,  8.38537481e-07,  8.38151494e-07]),\n",
       " 0.9008253818180506,\n",
       " {'grad': array([ 5.34144372e-07, -1.97833492e-07, -7.32105272e-09, ...,\n",
       "         -1.18395761e-10, -1.14568445e-10, -1.09296871e-10]),\n",
       "  'task': 'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL',\n",
       "  'funcalls': 4,\n",
       "  'nit': 2,\n",
       "  'warnflag': 0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy\n",
    "import numpy \n",
    "theta = np.hstack(([alpha],[0]*(nUsers+nItems)))\n",
    "#print(len(theta)) 164996\n",
    "labels = [d[2] for d in rating_data]\n",
    "lamb = 1\n",
    "scipy.optimize.fmin_l_bfgs_b(cost, theta, derivative, args = (labels, lamb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer for Problem9:\n",
    "\n",
    "a. MSE on the validation set :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"problem10.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"problem10.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum value of userBiases: 32445558 3.7003036684773426e-06\n",
      "minimum value of userBiases: 70705426 -1.2678107941701826e-06\n",
      "maximum value of itemBiases: 98124873 1.8437616886670768e-07\n",
      "minimum value of itemBiases: 29147042 -2.62689645997895e-07\n"
     ]
    }
   ],
   "source": [
    "userBiases_max = max(userBiases, key= lambda x: userBiases[x])\n",
    "print(\"maximum value of userBiases:\",userBiases_max, userBiases[userBiases_max])\n",
    "userBiases_min = min(userBiases, key= lambda x: userBiases[x])\n",
    "print(\"minimum value of userBiases:\",userBiases_min, userBiases[userBiases_min])\n",
    "itemBiases_max = max(itemBiases, key= lambda x: itemBiases[x])\n",
    "print(\"maximum value of itemBiases:\",itemBiases_max, itemBiases[itemBiases_max])\n",
    "itemBiases_min = min(itemBiases, key= lambda x: itemBiases[x])\n",
    "print(\"minimum value of itemBiases:\",itemBiases_min, itemBiases[itemBiases_min])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer for Problem10:\n",
    "\n",
    "a. user ID that have the largest value of beta : 32445558, value : 3.7003036684773426e-06\n",
    "\n",
    "b. user ID that have the smallest value of beta : 70705426, value : -1.2678107941701826e-06\n",
    "\n",
    "c. recipe ID that have the largest value of beta : 98124873, value : 1.8437616886670768e-07\n",
    "\n",
    "d. recipe ID that have the smallest value of beta : 29147042, value : -2.62689645997895e-07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"problem11.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"problem11.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lamb =  1e-05\n",
      "MSE = 0.9008923295603781\n",
      "MSE = 0.8880111212499086\n",
      "MSE = 1.069611774925789\n",
      "MSE = 0.8854287133812021\n",
      "MSE = 0.8800495326243006\n",
      "MSE = 0.8792288815381499\n",
      "MSE = 0.8761198198963718\n",
      "MSE = 0.8552503387512107\n",
      "MSE = 0.844030763897731\n",
      "MSE = 0.8287385168931087\n",
      "MSE = 0.8180375474910421\n",
      "MSE = 0.7979963501373877\n",
      "MSE = 0.7840140130811376\n",
      "MSE = 0.7711449292816135\n",
      "MSE = 0.7613025945278198\n",
      "MSE = 0.7507671736011382\n",
      "MSE = 5.345930082893958\n",
      "MSE = 0.7506959492153554\n",
      "MSE = 0.7467219604102234\n",
      "MSE = 0.7387969604882767\n",
      "MSE = 0.7338251748249827\n",
      "MSE = 0.7261427552356622\n",
      "MSE = 0.7185612868667165\n",
      "MSE = 0.7098738719307657\n",
      "MSE = 0.7009736748541021\n",
      "MSE = 0.6902352307829346\n",
      "MSE = 0.6820939765389208\n",
      "MSE = 0.6809004970478701\n",
      "MSE = 0.6806377856494903\n",
      "MSE = 0.6800865674487122\n",
      "MSE = 0.678011146489186\n",
      "MSE = 0.6759285824177541\n",
      "MSE = 0.6724558791953841\n",
      "MSE = 0.6714563598671184\n",
      "MSE = 0.6674767145274385\n",
      "MSE = 0.6684374809302834\n",
      "MSE = 0.6645768392321414\n",
      "MSE = 0.6613946514295945\n",
      "MSE = 0.6588469783561169\n",
      "MSE = 0.6552680178801122\n",
      "MSE = 0.6539755911054936\n",
      "MSE = 0.6543714316487619\n",
      "MSE = 0.6531478504559842\n",
      "MSE = 0.6531738264247221\n",
      "MSE = 0.6529455323919525\n",
      "MSE = 0.6535219415400308\n",
      "MSE = 0.6530414744047724\n",
      "MSE = 0.6527522885467963\n",
      "MSE = 0.6526004924888641\n",
      "MSE = 0.6514953316059208\n",
      "MSE = 0.651810231679115\n",
      "MSE = 0.6520867071200405\n",
      "MSE = 0.6521098608774898\n",
      "MSE = 0.6521126974666346\n",
      "MSE = 0.6531642611458578\n",
      "MSE = 0.6518599106408396\n",
      "MSE = 0.6516069778686175\n",
      "MSE = 0.6512596293787687\n",
      "MSE = 0.6511829470869952\n",
      "MSE = 0.6508753821953974\n",
      "MSE = 0.6505275168614008\n",
      "MSE = 0.6501753646144617\n",
      "MSE = 0.6498228235911943\n",
      "MSE = 0.649807975088368\n",
      "MSE = 0.6652478963758147\n",
      "MSE = 0.6497554666090166\n",
      "MSE = 0.6497079432578234\n",
      "MSE = 0.6496194653288437\n",
      "MSE = 0.6493059306629612\n",
      "MSE = 0.649118283358505\n",
      "MSE = 0.6490430164481296\n",
      "MSE = 0.6489093634430095\n",
      "MSE = 0.6488839070733912\n",
      "MSE = 0.6488500876007357\n",
      "MSE = 0.6475682814562694\n",
      "MSE = 0.6478068105953899\n",
      "MSE = 0.647731819273153\n",
      "MSE = 0.6475946766583445\n",
      "MSE = 0.6474794480709908\n",
      "MSE = 0.6462640955460146\n",
      "MSE = 0.6470189726939326\n",
      "MSE = 0.6572492238531034\n",
      "MSE = 0.6468564325407815\n",
      "MSE = 0.6467023375260005\n",
      "MSE = 0.6566834270930518\n",
      "MSE = 0.6468183801573265\n",
      "MSE = 0.6466180436784982\n",
      "MSE = 0.6463515641940021\n",
      "MSE = 0.6462976800216745\n",
      "MSE = 0.646234396488296\n",
      "MSE = 0.6461257112967079\n",
      "MSE = 0.6458761278634012\n",
      "MSE = 0.6455151938665773\n",
      "MSE = 0.6437553881100531\n",
      "MSE = 0.6448948171018407\n",
      "MSE = 0.6444063441935024\n",
      "MSE = 0.644624743793296\n",
      "MSE = 0.6446383576573183\n",
      "MSE = 0.6446563232787742\n",
      "MSE = 0.644557695665424\n",
      "MSE = 0.6441749125287669\n",
      "MSE = 0.6441362702641132\n",
      "MSE = 0.6440147217351855\n",
      "MSE = 0.6437541447091862\n",
      "MSE = 0.6435599628333424\n",
      "MSE = 0.6434353415035318\n",
      "MSE = 0.643357605861816\n",
      "MSE = 0.6432587830953387\n",
      "MSE = 0.6431719549755747\n",
      "MSE = 0.6428553117495017\n",
      "MSE = 0.6464380147726588\n",
      "MSE = 0.6428882362182476\n",
      "MSE = 0.6427652684251897\n",
      "MSE = 0.6426761407346012\n",
      "MSE = 0.6426075575934442\n",
      "MSE = 0.6424777431850217\n",
      "MSE = 0.6422846023501654\n",
      "MSE = 0.6424138868690303\n",
      "MSE = 0.6420828351389939\n",
      "MSE = 0.6417127169935639\n",
      "MSE = 0.6415730033940412\n",
      "MSE = 0.6414769715651176\n",
      "MSE = 0.641450693395469\n",
      "MSE = 0.6414390079183784\n",
      "MSE = 0.6415337276850459\n",
      "MSE = 0.6415654290388586\n",
      "MSE = 0.6415836794577897\n",
      "MSE = 0.6412634316419313\n",
      "MSE = 0.6415921834562589\n",
      "MSE = 0.6413793687111206\n",
      "MSE = 0.6411670124770998\n",
      "MSE = 0.6408587484932727\n",
      "MSE = 0.6407474739664187\n",
      "MSE = 0.6407974204303061\n",
      "MSE = 0.6403841670870936\n",
      "MSE = 0.6406170258398457\n",
      "MSE = 0.6406020960990854\n",
      "MSE = 0.6405919568344804\n",
      "MSE = 0.6404699671325975\n",
      "MSE = 0.6401402914929535\n",
      "MSE = 0.641437301097975\n",
      "MSE = 0.6401668302571034\n",
      "MSE = 0.6398702035651449\n",
      "MSE = 0.6453945389107121\n",
      "MSE = 0.6398309683157737\n",
      "MSE = 0.6395892447232924\n",
      "MSE = 0.6389924063628618\n",
      "MSE = 0.6392824221818221\n",
      "MSE = 0.6389800210128352\n",
      "MSE = 0.6381670807331046\n",
      "MSE = 0.6384894046474477\n",
      "MSE = 0.6384781967006361\n",
      "MSE = 0.6385447100234887\n",
      "MSE = 0.6381952040760621\n",
      "MSE = 0.6374711404205651\n",
      "MSE = 0.6390956106507142\n",
      "MSE = 0.6374457463115207\n",
      "MSE = 0.6369639311051181\n",
      "MSE = 0.6366839822392462\n",
      "MSE = 0.6369218345271305\n",
      "MSE = 0.636835706225587\n",
      "MSE = 0.6368147408372649\n",
      "MSE = 0.6368299360427108\n",
      "MSE = 0.636667140090628\n",
      "MSE = 0.6365166921374855\n",
      "MSE = 0.6363187847697235\n",
      "MSE = 0.6361851003850275\n",
      "MSE = 0.6361166061586757\n",
      "MSE = 0.636096575553414\n",
      "MSE = 0.6360908060416661\n",
      "MSE = 0.6368643067730128\n",
      "MSE = 0.6361281494428047\n",
      "MSE = 0.6361399781915908\n",
      "MSE = 0.6361921296837435\n",
      "MSE = 0.6362455980949782\n",
      "MSE = 0.6362558474694692\n",
      "MSE = 0.6360645912765704\n",
      "MSE = 0.6361688148498138\n",
      "MSE = 0.6360872803115023\n",
      "MSE = 0.635865509040343\n",
      "MSE = 0.6357759390949179\n",
      "MSE = 0.635938821633157\n",
      "MSE = 0.6357849968580424\n",
      "MSE = 0.6356697170487369\n",
      "MSE = 0.6357827838093234\n",
      "MSE = 0.6356902936432969\n",
      "MSE = 0.6356612016334707\n",
      "MSE = 0.635689548513651\n",
      "MSE = 0.6357132319490375\n",
      "MSE = 0.6356993501468186\n",
      "MSE = 0.6357120142191314\n",
      "MSE = 0.6356984366591759\n",
      "MSE = 0.6355253187283547\n",
      "MSE = 0.6355155602284275\n",
      "MSE = 0.6354121587066774\n",
      "MSE = 0.6353062759344451\n",
      "MSE = 0.6350512524944544\n",
      "MSE = 0.6351815887501837\n",
      "MSE = 0.6348770526228078\n",
      "MSE = 0.635148604590614\n",
      "MSE = 0.6350996952868735\n",
      "MSE = 0.6350883909708442\n",
      "MSE = 0.6351158323268064\n",
      "MSE = 0.6350848273329036\n",
      "MSE = 0.6350937480965854\n",
      "MSE = 0.6351165988534044\n",
      "MSE = 0.6351210247110727\n",
      "MSE = 0.6351279570205869\n",
      "MSE = 0.6344114400696008\n",
      "MSE = 0.6350105291776783\n",
      "MSE = 0.63499359886741\n",
      "MSE = 0.6349407010138794\n",
      "MSE = 0.634895949657667\n",
      "MSE = 0.6347321648650878\n",
      "MSE = 0.6348243888255913\n",
      "MSE = 0.6348167483492404\n",
      "MSE = 0.6348096345106009\n",
      "MSE = 0.6348086637135137\n",
      "MSE = 0.6348206143790429\n",
      "MSE = 0.6348100699449816\n",
      "MSE = 0.6348095136028468\n",
      "MSE = 0.6348039278835477\n",
      "MSE = 0.6348095330605363\n",
      "MSE = 0.6348058545986139\n",
      "MSE = 0.6347524836436221\n",
      "MSE = 0.6352762753776684\n",
      "MSE = 0.6347978479363615\n",
      "MSE = 0.6347780739758608\n",
      "MSE = 0.6347516485712363\n",
      "MSE = 0.6347667346207378\n",
      "MSE = 0.6347326792972932\n",
      "MSE = 0.6347163177930022\n",
      "MSE = 0.6346002956881384\n",
      "MSE = 0.634675748370534\n",
      "MSE = 0.6346938329560146\n",
      "MSE = 0.6347265616049144\n",
      "MSE = 0.6347255729761029\n",
      "MSE = 0.6347115577660264\n",
      "MSE = 0.6348671460047283\n",
      "MSE = 0.6347708569151191\n",
      "MSE = 0.6346148406915597\n",
      "MSE = 0.6346746083198328\n",
      "MSE = 0.6346892742224873\n",
      "MSE = 0.634702807893328\n",
      "MSE = 0.6342540772423353\n",
      "MSE = 0.634676624070215\n",
      "MSE = 0.6347037256792801\n",
      "MSE = 0.6346729338686231\n",
      "MSE = 0.6346481929112168\n",
      "MSE = 0.6346259367457087\n",
      "MSE = 0.6346171282646317\n",
      "MSE = 0.6346105822515712\n",
      "MSE = 0.6346047398113261\n",
      "MSE = 0.6345848473256432\n",
      "MSE = 0.6345534557006808\n",
      "MSE = 0.6338886915560242\n",
      "MSE = 0.6345117545536385\n",
      "MSE = 0.6343841970459558\n",
      "MSE = 0.6344653665413593\n",
      "MSE = 0.6344629956910439\n",
      "MSE = 0.6344431420836587\n",
      "MSE = 0.6344371539215351\n",
      "MSE = 0.6344284278942932\n",
      "MSE = 0.6344158840617314\n",
      "MSE = 0.6344427829622108\n",
      "MSE = 0.634418119340311\n",
      "MSE = 0.634407061274982\n",
      "MSE = 0.6344012977194741\n",
      "MSE = 0.6344003035124637\n",
      "MSE = 0.6343986803760061\n",
      "MSE = 0.6343909249757739\n",
      "MSE = 0.634372447742189\n",
      "MSE = 0.6343265598938645\n",
      "MSE = 0.6343662262078718\n",
      "MSE = 0.6343280804301162\n",
      "MSE = 0.6342934667617972\n",
      "MSE = 0.6342744840271014\n",
      "MSE = 0.6342679235131122\n",
      "MSE = 0.6342488452212572\n",
      "MSE = 0.6342622067223109\n",
      "MSE = 0.6342507404416353\n",
      "MSE = 0.6342312448754917\n",
      "MSE = 0.6342140040996228\n",
      "MSE = 0.6342116412359292\n",
      "MSE = 0.6341291575505147\n",
      "MSE = 0.6341851607405737\n",
      "MSE = 0.634202345013962\n",
      "MSE = 0.6341880632397595\n",
      "MSE = 0.6341913193545488\n",
      "MSE = 0.6341947955073256\n",
      "MSE = 0.6340009684145611\n",
      "MSE = 0.6341715750487976\n",
      "MSE = 0.6341628376078816\n",
      "MSE = 0.6341514918490772\n",
      "MSE = 0.6341294078022928\n",
      "MSE = 0.6341183840468194\n",
      "MSE = 0.6341386696833684\n",
      "MSE = 0.6341157730039024\n",
      "MSE = 0.6340993258003779\n",
      "MSE = 0.6340823961292988\n",
      "MSE = 0.6340841848833918\n",
      "MSE = 0.6338827757059475\n",
      "MSE = 0.6340772928867582\n",
      "MSE = 0.634081380401378\n",
      "MSE = 0.6340657003597402\n",
      "MSE = 0.6340710466695122\n",
      "MSE = 0.6340779066565423\n",
      "MSE = 0.6340891966040787\n",
      "MSE = 0.6340802247293351\n",
      "MSE = 0.6340763386987056\n",
      "MSE = 0.6340714184032632\n",
      "MSE = 0.6340750815206039\n",
      "MSE = 0.6340669554401532\n",
      "MSE = 0.6340630783795247\n",
      "MSE = 0.6340597354086867\n",
      "MSE = 0.6340702099180981\n",
      "MSE = 0.6342934036366215\n",
      "MSE = 0.6340801018729273\n",
      "MSE = 0.6341045422585108\n",
      "MSE = 0.6340869138720852\n",
      "MSE = 0.6341007662138642\n",
      "MSE = 0.6341134327733907\n",
      "MSE = 0.6341235802878193\n",
      "MSE = 0.634136497156714\n",
      "MSE = 0.6341883370776465\n",
      "MSE = 0.6341383460311428\n",
      "MSE = 0.6341457890983845\n",
      "MSE = 0.6341553033299521\n",
      "MSE = 0.6341525961754956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 0.634145990024387\n",
      "MSE = 0.6341634033962597\n",
      "MSE = 0.6341469348284917\n",
      "MSE = 0.6341452700570915\n",
      "MSE = 0.6341473471083766\n",
      "MSE = 0.6341167133131254\n",
      "MSE = 0.634141858898768\n",
      "MSE = 0.6341444278273037\n",
      "MSE = 0.6341526355067247\n",
      "MSE = 0.634154020626509\n",
      "MSE = 0.634150240106463\n",
      "MSE = 0.6341477487661004\n",
      "MSE = 0.634145570355057\n",
      "MSE = 0.6341447120348698\n",
      "MSE = 0.6341463156870584\n",
      "MSE = 0.6341380505559471\n",
      "MSE = 0.6341449388211792\n",
      "MSE = 0.6341481788010801\n",
      "MSE = 0.634154497656838\n",
      "MSE = 0.6341558386685454\n",
      "MSE = 0.6341637617328665\n",
      "MSE = 0.634150696594303\n",
      "MSE = 0.6341612331188466\n",
      "MSE = 0.6341645257273949\n",
      "MSE = 0.6341671913991425\n",
      "MSE = 0.6341687176764904\n",
      "MSE = 0.6341680289471773\n",
      "MSE = 0.6341742650069225\n",
      "MSE = 0.634177277246967\n",
      "MSE = 0.6341978263757083\n",
      "MSE = 0.6341859919882981\n",
      "MSE = 0.6341923920606669\n",
      "MSE = 0.6341900242523433\n",
      "MSE = 0.6341886618744861\n",
      "MSE = 0.6341885776394552\n",
      "MSE = 0.6341829802146746\n",
      "MSE = 0.6341906479203796\n",
      "MSE = 0.6341951836312085\n",
      "MSE = 0.6342176687400954\n",
      "MSE = 0.6341981711512272\n",
      "MSE = 0.6342059525417488\n",
      "MSE = 0.6342085135199366\n",
      "MSE = 0.634216546397375\n",
      "MSE = 0.6342175410616744\n",
      "MSE = 0.6342178465119325\n",
      "MSE = 0.6342187558052469\n",
      "MSE = 0.6342205535718703\n",
      "MSE = 0.6342228663976861\n",
      "MSE = 0.6342332172050339\n",
      "MSE = 0.6342303027493539\n",
      "MSE = 0.6342296344512754\n",
      "MSE = 0.6342323064787018\n",
      "MSE = 0.6342343922378433\n",
      "MSE = 0.6342451866298886\n",
      "MSE = 0.6342452347935369\n",
      "MSE = 0.6342450632205238\n",
      "MSE = 0.634245037715461\n",
      "MSE = 0.6342513479234594\n",
      "MSE = 0.6342382024011222\n",
      "MSE = 0.6342482162347715\n",
      "MSE = 0.6342512387196795\n",
      "MSE = 0.6342579474901072\n",
      "MSE = 0.634268733921549\n",
      "MSE = 0.6342598115125848\n",
      "MSE = 0.6342623472025214\n",
      "MSE = 0.6342688243081908\n",
      "MSE = 0.6342656867380435\n",
      "MSE = 0.6342669840684998\n",
      "MSE = 0.6342659539636893\n",
      "MSE = 0.6342878454516789\n",
      "MSE = 0.6342663899331522\n",
      "MSE = 0.6342660707701869\n",
      "MSE = 0.6342688102643227\n",
      "MSE = 0.6342746137585848\n",
      "MSE = 0.6342857179476071\n",
      "MSE = 0.6342995876914765\n",
      "MSE = 0.6343253154136154\n",
      "MSE = 0.6343510523696653\n",
      "MSE = 0.6343333543712552\n",
      "MSE = 0.6343295687227958\n",
      "MSE = 0.6343230711858318\n",
      "MSE = 0.6343270033693744\n",
      "MSE = 0.6343308205994943\n",
      "lamb =  0.0001\n",
      "MSE = 0.9008923295603781\n",
      "MSE = 0.8880111212499086\n",
      "MSE = 1.06862279077\n",
      "MSE = 0.8855270897361742\n",
      "MSE = 0.8800733442598857\n",
      "MSE = 0.8792401321678299\n",
      "MSE = 0.8760801200129987\n",
      "MSE = 0.85543794918346\n",
      "MSE = 0.8447671461635389\n",
      "MSE = 0.8297331772329899\n",
      "MSE = 0.8204837554157354\n",
      "MSE = 0.8065773473043875\n",
      "MSE = 0.7976501064097553\n",
      "MSE = 0.7942577438023959\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/x9/xv9fr3td34x85pl4rz2hy7kh0000gn/T/ipykernel_23443/3993606945.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlamb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.00001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lamb = \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlamb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmin_l_bfgs_b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mderivative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlamb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"predictions_Rated_bias_%f.txt\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlamb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfmin_l_bfgs_b\u001b[0;34m(func, x0, fprime, args, approx_grad, bounds, m, factr, pgtol, epsilon, iprint, maxfun, maxiter, disp, callback, maxls)\u001b[0m\n\u001b[1;32m    195\u001b[0m             'maxls': maxls}\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     res = _minimize_lbfgsb(fun, x0, args=args, jac=jac, bounds=bounds,\n\u001b[0m\u001b[1;32m    198\u001b[0m                            **opts)\n\u001b[1;32m    199\u001b[0m     d = {'grad': res['jac'],\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'NEW_X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_and_grad\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_x_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36m_update_grad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_update_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg_updated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_grad_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg_updated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mupdate_grad\u001b[0;34m()\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mupdate_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mFD_METHODS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mgrad_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mgrad_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngev\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mupdate_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/x9/xv9fr3td34x85pl4rz2hy7kh0000gn/T/ipykernel_23443/860518651.py\u001b[0m in \u001b[0;36mderivative\u001b[0;34m(theta, labels, lamb)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mdalpha\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mdUserBiases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mdItemBiases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mu\u001b[0m \u001b[0;32min\u001b[0m \u001b[0muserBiases\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# MSE: 0.4547707140445709 -> MSE: 0.45477062463760376\n",
    "\n",
    "for lamb in [0.00001, 0.0001] :\n",
    "    print(\"lamb = \",lamb)\n",
    "    scipy.optimize.fmin_l_bfgs_b(cost, theta, derivative, args = (labels, lamb))\n",
    "    \n",
    "    predictions = open(\"predictions_Rated_bias_%f.txt\" % lamb,'w')\n",
    "    for l in open(\"stub_Rated.txt\"):\n",
    "        if l.startswith(\"user_id\"):\n",
    "            #header\n",
    "            predictions.write(l)\n",
    "            continue\n",
    "        u,i = l.strip().split('-')\n",
    "        pred = alpha\n",
    "        if u in userBiases.keys():\n",
    "            pred+=userBiases[u]\n",
    "        if i in itemBiases.keys():\n",
    "            pred+=itemBiases[i]\n",
    "        predictions.write(u + '-' + i + \",%f\\n\"%pred)\n",
    "    predictions.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lamb =  1e-05\n",
      "MSE = 0.9008923295603781\n",
      "MSE = 0.8880111212499086\n",
      "MSE = 1.069611774925789\n",
      "MSE = 0.8854287133812021\n",
      "MSE = 0.8800495326243006\n",
      "MSE = 0.8792288815381499\n",
      "MSE = 0.8761198198963718\n",
      "MSE = 0.8552503387512107\n",
      "MSE = 0.844030763897731\n",
      "MSE = 0.8287385168931087\n",
      "MSE = 0.8180375474910421\n",
      "MSE = 0.7979963501373877\n",
      "MSE = 0.7840140130811376\n",
      "MSE = 0.7711449292816135\n",
      "MSE = 0.7613025945278198\n",
      "MSE = 0.7507671736011382\n",
      "MSE = 5.345930082893958\n",
      "MSE = 0.7506959492153554\n",
      "MSE = 0.7467219604102234\n",
      "MSE = 0.7387969604882767\n",
      "MSE = 0.7338251748249827\n",
      "MSE = 0.7261427552356622\n",
      "MSE = 0.7185612868667165\n",
      "MSE = 0.7098738719307657\n",
      "MSE = 0.7009736748541021\n",
      "MSE = 0.6902352307829346\n",
      "MSE = 0.6820939765389208\n",
      "MSE = 0.6809004970478701\n",
      "MSE = 0.6806377856494903\n",
      "MSE = 0.6800865674487122\n",
      "MSE = 0.678011146489186\n",
      "MSE = 0.6759285824177541\n",
      "MSE = 0.6724558791953841\n",
      "MSE = 0.6714563598671184\n",
      "MSE = 0.6674767145274385\n",
      "MSE = 0.6684374809302834\n",
      "MSE = 0.6645768392321414\n",
      "MSE = 0.6613946514295945\n",
      "MSE = 0.6588469783561169\n",
      "MSE = 0.6552680178801122\n",
      "MSE = 0.6539755911054936\n",
      "MSE = 0.6543714316487619\n",
      "MSE = 0.6531478504559842\n",
      "MSE = 0.6531738264247221\n",
      "MSE = 0.6529455323919525\n",
      "MSE = 0.6535219415400308\n",
      "MSE = 0.6530414744047724\n",
      "MSE = 0.6527522885467963\n",
      "MSE = 0.6526004924888641\n",
      "MSE = 0.6514953316059208\n",
      "MSE = 0.651810231679115\n",
      "MSE = 0.6520867071200405\n",
      "MSE = 0.6521098608774898\n",
      "MSE = 0.6521126974666346\n",
      "MSE = 0.6531642611458578\n",
      "MSE = 0.6518599106408396\n",
      "MSE = 0.6516069778686175\n",
      "MSE = 0.6512596293787687\n",
      "MSE = 0.6511829470869952\n",
      "MSE = 0.6508753821953974\n",
      "MSE = 0.6505275168614008\n",
      "MSE = 0.6501753646144617\n",
      "MSE = 0.6498228235911943\n",
      "MSE = 0.649807975088368\n",
      "MSE = 0.6652478963758147\n",
      "MSE = 0.6497554666090166\n",
      "MSE = 0.6497079432578234\n",
      "MSE = 0.6496194653288437\n",
      "MSE = 0.6493059306629612\n",
      "MSE = 0.649118283358505\n",
      "MSE = 0.6490430164481296\n",
      "MSE = 0.6489093634430095\n",
      "MSE = 0.6488839070733912\n",
      "MSE = 0.6488500876007357\n",
      "MSE = 0.6475682814562694\n",
      "MSE = 0.6478068105953899\n",
      "MSE = 0.647731819273153\n",
      "MSE = 0.6475946766583445\n",
      "MSE = 0.6474794480709908\n",
      "MSE = 0.6462640955460146\n",
      "MSE = 0.6470189726939326\n",
      "MSE = 0.6572492238531034\n",
      "MSE = 0.6468564325407815\n",
      "MSE = 0.6467023375260005\n",
      "MSE = 0.6566834270930518\n",
      "MSE = 0.6468183801573265\n",
      "MSE = 0.6466180436784982\n",
      "MSE = 0.6463515641940021\n",
      "MSE = 0.6462976800216745\n",
      "MSE = 0.646234396488296\n",
      "MSE = 0.6461257112967079\n",
      "MSE = 0.6458761278634012\n",
      "MSE = 0.6455151938665773\n",
      "MSE = 0.6437553881100531\n",
      "MSE = 0.6448948171018407\n",
      "MSE = 0.6444063441935024\n",
      "MSE = 0.644624743793296\n",
      "MSE = 0.6446383576573183\n",
      "MSE = 0.6446563232787742\n",
      "MSE = 0.644557695665424\n",
      "MSE = 0.6441749125287669\n",
      "MSE = 0.6441362702641132\n",
      "MSE = 0.6440147217351855\n",
      "MSE = 0.6437541447091862\n",
      "MSE = 0.6435599628333424\n",
      "MSE = 0.6434353415035318\n",
      "MSE = 0.643357605861816\n",
      "MSE = 0.6432587830953387\n",
      "MSE = 0.6431719549755747\n",
      "MSE = 0.6428553117495017\n",
      "MSE = 0.6464380147726588\n",
      "MSE = 0.6428882362182476\n",
      "MSE = 0.6427652684251897\n",
      "MSE = 0.6426761407346012\n",
      "MSE = 0.6426075575934442\n",
      "MSE = 0.6424777431850217\n",
      "MSE = 0.6422846023501654\n",
      "MSE = 0.6424138868690303\n",
      "MSE = 0.6420828351389939\n",
      "MSE = 0.6417127169935639\n",
      "MSE = 0.6415730033940412\n",
      "MSE = 0.6414769715651176\n",
      "MSE = 0.641450693395469\n",
      "MSE = 0.6414390079183784\n",
      "MSE = 0.6415337276850459\n",
      "MSE = 0.6415654290388586\n",
      "MSE = 0.6415836794577897\n",
      "MSE = 0.6412634316419313\n",
      "MSE = 0.6415921834562589\n",
      "MSE = 0.6413793687111206\n",
      "MSE = 0.6411670124770998\n",
      "MSE = 0.6408587484932727\n",
      "MSE = 0.6407474739664187\n",
      "MSE = 0.6407974204303061\n",
      "MSE = 0.6403841670870936\n",
      "MSE = 0.6406170258398457\n",
      "MSE = 0.6406020960990854\n",
      "MSE = 0.6405919568344804\n",
      "MSE = 0.6404699671325975\n",
      "MSE = 0.6401402914929535\n",
      "MSE = 0.641437301097975\n",
      "MSE = 0.6401668302571034\n",
      "MSE = 0.6398702035651449\n",
      "MSE = 0.6453945389107121\n",
      "MSE = 0.6398309683157737\n",
      "MSE = 0.6395892447232924\n",
      "MSE = 0.6389924063628618\n",
      "MSE = 0.6392824221818221\n",
      "MSE = 0.6389800210128352\n",
      "MSE = 0.6381670807331046\n",
      "MSE = 0.6384894046474477\n",
      "MSE = 0.6384781967006361\n",
      "MSE = 0.6385447100234887\n",
      "MSE = 0.6381952040760621\n",
      "MSE = 0.6374711404205651\n",
      "MSE = 0.6390956106507142\n",
      "MSE = 0.6374457463115207\n",
      "MSE = 0.6369639311051181\n",
      "MSE = 0.6366839822392462\n",
      "MSE = 0.6369218345271305\n",
      "MSE = 0.636835706225587\n",
      "MSE = 0.6368147408372649\n",
      "MSE = 0.6368299360427108\n",
      "MSE = 0.636667140090628\n",
      "MSE = 0.6365166921374855\n",
      "MSE = 0.6363187847697235\n",
      "MSE = 0.6361851003850275\n",
      "MSE = 0.6361166061586757\n",
      "MSE = 0.636096575553414\n",
      "MSE = 0.6360908060416661\n",
      "MSE = 0.6368643067730128\n",
      "MSE = 0.6361281494428047\n",
      "MSE = 0.6361399781915908\n",
      "MSE = 0.6361921296837435\n",
      "MSE = 0.6362455980949782\n",
      "MSE = 0.6362558474694692\n",
      "MSE = 0.6360645912765704\n",
      "MSE = 0.6361688148498138\n",
      "MSE = 0.6360872803115023\n",
      "MSE = 0.635865509040343\n",
      "MSE = 0.6357759390949179\n",
      "MSE = 0.635938821633157\n",
      "MSE = 0.6357849968580424\n",
      "MSE = 0.6356697170487369\n",
      "MSE = 0.6357827838093234\n",
      "MSE = 0.6356902936432969\n",
      "MSE = 0.6356612016334707\n",
      "MSE = 0.635689548513651\n",
      "MSE = 0.6357132319490375\n",
      "MSE = 0.6356993501468186\n",
      "MSE = 0.6357120142191314\n",
      "MSE = 0.6356984366591759\n",
      "MSE = 0.6355253187283547\n",
      "MSE = 0.6355155602284275\n",
      "MSE = 0.6354121587066774\n",
      "MSE = 0.6353062759344451\n",
      "MSE = 0.6350512524944544\n",
      "MSE = 0.6351815887501837\n",
      "MSE = 0.6348770526228078\n",
      "MSE = 0.635148604590614\n",
      "MSE = 0.6350996952868735\n",
      "MSE = 0.6350883909708442\n",
      "MSE = 0.6351158323268064\n",
      "MSE = 0.6350848273329036\n",
      "MSE = 0.6350937480965854\n",
      "MSE = 0.6351165988534044\n",
      "MSE = 0.6351210247110727\n",
      "MSE = 0.6351279570205869\n",
      "MSE = 0.6344114400696008\n",
      "MSE = 0.6350105291776783\n",
      "MSE = 0.63499359886741\n",
      "MSE = 0.6349407010138794\n",
      "MSE = 0.634895949657667\n",
      "MSE = 0.6347321648650878\n",
      "MSE = 0.6348243888255913\n",
      "MSE = 0.6348167483492404\n",
      "MSE = 0.6348096345106009\n",
      "MSE = 0.6348086637135137\n",
      "MSE = 0.6348206143790429\n",
      "MSE = 0.6348100699449816\n",
      "MSE = 0.6348095136028468\n",
      "MSE = 0.6348039278835477\n",
      "MSE = 0.6348095330605363\n",
      "MSE = 0.6348058545986139\n",
      "MSE = 0.6347524836436221\n",
      "MSE = 0.6352762753776684\n",
      "MSE = 0.6347978479363615\n",
      "MSE = 0.6347780739758608\n",
      "MSE = 0.6347516485712363\n",
      "MSE = 0.6347667346207378\n",
      "MSE = 0.6347326792972932\n",
      "MSE = 0.6347163177930022\n",
      "MSE = 0.6346002956881384\n",
      "MSE = 0.634675748370534\n",
      "MSE = 0.6346938329560146\n",
      "MSE = 0.6347265616049144\n",
      "MSE = 0.6347255729761029\n",
      "MSE = 0.6347115577660264\n",
      "MSE = 0.6348671460047283\n",
      "MSE = 0.6347708569151191\n",
      "MSE = 0.6346148406915597\n",
      "MSE = 0.6346746083198328\n",
      "MSE = 0.6346892742224873\n",
      "MSE = 0.634702807893328\n",
      "MSE = 0.6342540772423353\n",
      "MSE = 0.634676624070215\n",
      "MSE = 0.6347037256792801\n",
      "MSE = 0.6346729338686231\n",
      "MSE = 0.6346481929112168\n",
      "MSE = 0.6346259367457087\n",
      "MSE = 0.6346171282646317\n",
      "MSE = 0.6346105822515712\n",
      "MSE = 0.6346047398113261\n",
      "MSE = 0.6345848473256432\n",
      "MSE = 0.6345534557006808\n",
      "MSE = 0.6338886915560242\n",
      "MSE = 0.6345117545536385\n",
      "MSE = 0.6343841970459558\n",
      "MSE = 0.6344653665413593\n",
      "MSE = 0.6344629956910439\n",
      "MSE = 0.6344431420836587\n",
      "MSE = 0.6344371539215351\n",
      "MSE = 0.6344284278942932\n",
      "MSE = 0.6344158840617314\n",
      "MSE = 0.6344427829622108\n",
      "MSE = 0.634418119340311\n",
      "MSE = 0.634407061274982\n",
      "MSE = 0.6344012977194741\n",
      "MSE = 0.6344003035124637\n",
      "MSE = 0.6343986803760061\n",
      "MSE = 0.6343909249757739\n",
      "MSE = 0.634372447742189\n",
      "MSE = 0.6343265598938645\n",
      "MSE = 0.6343662262078718\n",
      "MSE = 0.6343280804301162\n",
      "MSE = 0.6342934667617972\n",
      "MSE = 0.6342744840271014\n",
      "MSE = 0.6342679235131122\n",
      "MSE = 0.6342488452212572\n",
      "MSE = 0.6342622067223109\n",
      "MSE = 0.6342507404416353\n",
      "MSE = 0.6342312448754917\n",
      "MSE = 0.6342140040996228\n",
      "MSE = 0.6342116412359292\n",
      "MSE = 0.6341291575505147\n",
      "MSE = 0.6341851607405737\n",
      "MSE = 0.634202345013962\n",
      "MSE = 0.6341880632397595\n",
      "MSE = 0.6341913193545488\n",
      "MSE = 0.6341947955073256\n",
      "MSE = 0.6340009684145611\n",
      "MSE = 0.6341715750487976\n",
      "MSE = 0.6341628376078816\n",
      "MSE = 0.6341514918490772\n",
      "MSE = 0.6341294078022928\n",
      "MSE = 0.6341183840468194\n",
      "MSE = 0.6341386696833684\n",
      "MSE = 0.6341157730039024\n",
      "MSE = 0.6340993258003779\n",
      "MSE = 0.6340823961292988\n",
      "MSE = 0.6340841848833918\n",
      "MSE = 0.6338827757059475\n",
      "MSE = 0.6340772928867582\n",
      "MSE = 0.634081380401378\n",
      "MSE = 0.6340657003597402\n",
      "MSE = 0.6340710466695122\n",
      "MSE = 0.6340779066565423\n",
      "MSE = 0.6340891966040787\n",
      "MSE = 0.6340802247293351\n",
      "MSE = 0.6340763386987056\n",
      "MSE = 0.6340714184032632\n",
      "MSE = 0.6340750815206039\n",
      "MSE = 0.6340669554401532\n",
      "MSE = 0.6340630783795247\n",
      "MSE = 0.6340597354086867\n",
      "MSE = 0.6340702099180981\n",
      "MSE = 0.6342934036366215\n",
      "MSE = 0.6340801018729273\n",
      "MSE = 0.6341045422585108\n",
      "MSE = 0.6340869138720852\n",
      "MSE = 0.6341007662138642\n",
      "MSE = 0.6341134327733907\n",
      "MSE = 0.6341235802878193\n",
      "MSE = 0.634136497156714\n",
      "MSE = 0.6341883370776465\n",
      "MSE = 0.6341383460311428\n",
      "MSE = 0.6341457890983845\n",
      "MSE = 0.6341553033299521\n",
      "MSE = 0.6341525961754956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 0.634145990024387\n",
      "MSE = 0.6341634033962597\n",
      "MSE = 0.6341469348284917\n",
      "MSE = 0.6341452700570915\n",
      "MSE = 0.6341473471083766\n",
      "MSE = 0.6341167133131254\n",
      "MSE = 0.634141858898768\n",
      "MSE = 0.6341444278273037\n",
      "MSE = 0.6341526355067247\n",
      "MSE = 0.634154020626509\n",
      "MSE = 0.634150240106463\n",
      "MSE = 0.6341477487661004\n",
      "MSE = 0.634145570355057\n",
      "MSE = 0.6341447120348698\n",
      "MSE = 0.6341463156870584\n",
      "MSE = 0.6341380505559471\n",
      "MSE = 0.6341449388211792\n",
      "MSE = 0.6341481788010801\n",
      "MSE = 0.634154497656838\n",
      "MSE = 0.6341558386685454\n",
      "MSE = 0.6341637617328665\n",
      "MSE = 0.634150696594303\n",
      "MSE = 0.6341612331188466\n",
      "MSE = 0.6341645257273949\n",
      "MSE = 0.6341671913991425\n",
      "MSE = 0.6341687176764904\n",
      "MSE = 0.6341680289471773\n",
      "MSE = 0.6341742650069225\n",
      "MSE = 0.634177277246967\n",
      "MSE = 0.6341978263757083\n",
      "MSE = 0.6341859919882981\n",
      "MSE = 0.6341923920606669\n",
      "MSE = 0.6341900242523433\n",
      "MSE = 0.6341886618744861\n",
      "MSE = 0.6341885776394552\n",
      "MSE = 0.6341829802146746\n",
      "MSE = 0.6341906479203796\n",
      "MSE = 0.6341951836312085\n",
      "MSE = 0.6342176687400954\n",
      "MSE = 0.6341981711512272\n",
      "MSE = 0.6342059525417488\n",
      "MSE = 0.6342085135199366\n",
      "MSE = 0.634216546397375\n",
      "MSE = 0.6342175410616744\n",
      "MSE = 0.6342178465119325\n",
      "MSE = 0.6342187558052469\n",
      "MSE = 0.6342205535718703\n",
      "MSE = 0.6342228663976861\n",
      "MSE = 0.6342332172050339\n",
      "MSE = 0.6342303027493539\n",
      "MSE = 0.6342296344512754\n",
      "MSE = 0.6342323064787018\n",
      "MSE = 0.6342343922378433\n",
      "MSE = 0.6342451866298886\n",
      "MSE = 0.6342452347935369\n",
      "MSE = 0.6342450632205238\n",
      "MSE = 0.634245037715461\n",
      "MSE = 0.6342513479234594\n",
      "MSE = 0.6342382024011222\n",
      "MSE = 0.6342482162347715\n",
      "MSE = 0.6342512387196795\n",
      "MSE = 0.6342579474901072\n",
      "MSE = 0.634268733921549\n",
      "MSE = 0.6342598115125848\n",
      "MSE = 0.6342623472025214\n",
      "MSE = 0.6342688243081908\n",
      "MSE = 0.6342656867380435\n",
      "MSE = 0.6342669840684998\n",
      "MSE = 0.6342659539636893\n",
      "MSE = 0.6342878454516789\n",
      "MSE = 0.6342663899331522\n",
      "MSE = 0.6342660707701869\n",
      "MSE = 0.6342688102643227\n",
      "MSE = 0.6342746137585848\n",
      "MSE = 0.6342857179476071\n",
      "MSE = 0.6342995876914765\n",
      "MSE = 0.6343253154136154\n",
      "MSE = 0.6343510523696653\n",
      "MSE = 0.6343333543712552\n",
      "MSE = 0.6343295687227958\n",
      "MSE = 0.6343230711858318\n",
      "MSE = 0.6343270033693744\n",
      "MSE = 0.6343308205994943\n",
      "lamb =  0.0001\n",
      "MSE = 0.9008923295603781\n",
      "MSE = 0.8880111212499086\n",
      "MSE = 1.06862279077\n",
      "MSE = 0.8855270897361742\n",
      "MSE = 0.8800733442598857\n",
      "MSE = 0.8792401321678299\n",
      "MSE = 0.8760801200129987\n",
      "MSE = 0.85543794918346\n",
      "MSE = 0.8447671461635389\n",
      "MSE = 0.8297331772329899\n",
      "MSE = 0.8204837554157354\n",
      "MSE = 0.8065773473043875\n",
      "MSE = 0.7976501064097553\n",
      "MSE = 0.7942577438023959\n",
      "MSE = 0.7927018130999864\n",
      "MSE = 0.7905653925745705\n",
      "MSE = 0.8512358769363845\n",
      "MSE = 0.790515783166518\n",
      "MSE = 0.7874443580766012\n",
      "MSE = 0.7849211547156516\n",
      "MSE = 0.7828499720498525\n",
      "MSE = 0.7817458649577731\n",
      "MSE = 0.7812932943269393\n",
      "MSE = 0.7816922802022033\n",
      "MSE = 0.7815845470920835\n",
      "MSE = 0.7812976190180073\n",
      "MSE = 0.7806994521071291\n",
      "MSE = 0.7802859419157254\n",
      "MSE = 0.780265547372516\n",
      "MSE = 0.7802750756929726\n",
      "MSE = 0.7803137870650587\n",
      "MSE = 0.7798456511351345\n",
      "MSE = 0.7800902021799689\n",
      "MSE = 0.7801480192103055\n",
      "MSE = 0.78010359241491\n",
      "MSE = 0.7800419460191758\n",
      "MSE = 0.7799744959900314\n",
      "MSE = 0.7799250986224038\n",
      "MSE = 0.7799175040116846\n",
      "MSE = 0.7799189169771109\n",
      "MSE = 0.7799399996321964\n",
      "MSE = 0.7802509677831934\n",
      "MSE = 0.7799410324002617\n",
      "MSE = 0.779952286292096\n",
      "MSE = 0.7801414106869669\n",
      "MSE = 0.7799693706079754\n",
      "MSE = 0.7799719895664099\n",
      "MSE = 0.7799934607568919\n",
      "MSE = 0.7800080167203193\n",
      "MSE = 0.7800174629415336\n",
      "lamb =  0.001\n",
      "MSE = 0.9008923295603781\n",
      "MSE = 0.8880111212499086\n",
      "MSE = 1.0607060073644645\n",
      "MSE = 0.8863010253223489\n",
      "MSE = 0.881258376036975\n",
      "MSE = 0.8803560986339667\n",
      "MSE = 0.8769389926187163\n",
      "MSE = 0.8643200138886875\n",
      "MSE = 0.8606151882312933\n",
      "MSE = 0.857579086062278\n",
      "MSE = 0.8574139564458994\n",
      "MSE = 0.857512232280082\n",
      "MSE = 0.857484544764681\n",
      "MSE = 0.8574310680388667\n",
      "MSE = 0.8573958898569245\n",
      "MSE = 0.8573993276748094\n",
      "MSE = 0.8573996194370783\n",
      "MSE = 0.8574021993238108\n",
      "MSE = 0.8574001821737313\n",
      "MSE = 0.8574004686509782\n"
     ]
    }
   ],
   "source": [
    "for lamb in [0.00001, 0.0001, 0.001] :\n",
    "    print(\"lamb = \",lamb)\n",
    "    scipy.optimize.fmin_l_bfgs_b(cost, theta, derivative, args = (labels, lamb))\n",
    "    \n",
    "    predictions = open(\"predictions_Rated_bias_%f_20000.txt\" % lamb,'w')\n",
    "    for l in open(\"stub_Made.txt\"):\n",
    "        if l.startswith(\"user_id\"):\n",
    "            #header\n",
    "            predictions.write(l)\n",
    "            continue\n",
    "        u,i = l.strip().split('-')\n",
    "        pred = alpha\n",
    "        if u in userBiases.keys():\n",
    "            pred+=userBiases[u]\n",
    "        if i in itemBiases.keys():\n",
    "            pred+=itemBiases[i]\n",
    "        predictions.write(u + '-' + i + \",%f\\n\"%pred)\n",
    "    predictions.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lamb in [100, 1000] :\n",
    "    print(\"lamb = \",lamb)\n",
    "    scipy.optimize.fmin_l_bfgs_b(cost, theta, derivative, args = (labels, lamb))\n",
    "    \n",
    "    predictions = open(\"predictions_Rated_bias_%f.txt\" % lamb,'w')\n",
    "    for l in open(\"stub_Rated.txt\"):\n",
    "        if l.startswith(\"user_id\"):\n",
    "            #header\n",
    "            predictions.write(l)\n",
    "            continue\n",
    "        u,i = l.strip().split('-')\n",
    "        pred = alpha\n",
    "        if u in userBiases.keys():\n",
    "            pred+=userBiases[u]\n",
    "        if i in itemBiases.keys():\n",
    "            pred+=itemBiases[i]\n",
    "        predictions.write(u + '-' + i + \",%f\\n\"%pred)\n",
    "    predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-8.76262562739984e-25\n",
      "-7.549711999005067e-26\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "user_bias_mean = np.array(list(userBiases.values())).mean()\n",
    "item_bias_mean = np.array(list(itemBiases.values())).mean()\n",
    "print(user_bias_mean)\n",
    "print(item_bias_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer for Problem11:\n",
    "\n",
    "a. value lamb wrt MSE :\n",
    "\n",
    "lamb = 1e-05, MSE = 0.6343308205994943\n",
    "lamb =  0.0001, MSE = 0.7800174629415336\n",
    "lamb =  0.001, MSE = 0.8574004686509782\n",
    "lamb = 0.01, MSE = 0.8911307388311662\n",
    "lamb =  0.1, MSE = 0.8996097642865012\n",
    "lamb = 10, MSE = 0.9008789032254846\n",
    "lamb =  100, MSE = 0.9008909863002668\n",
    "lamb = 1000, MSE = 0.900892195231244"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
